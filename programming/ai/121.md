Learn the concept of Recommendation Systems and potential business applications

Here's a breakdown of how you can learn about recommendation systems with a special focus on their business applications:

**Learning the Concepts**

* **Online Courses:** Coursea, Udemy, and edX offer introductory and advanced courses on recommendation systems, which cover:  
    * Different types of recommendation systems (content-based, collaborative filtering, hybrid)
    * Algorithms and techniques used (matrix factorization, k-nearest neighbors, etc.)
    * How to evaluate the effectiveness of a recommendation system
* **Technical Articles and Blog Posts:** Many publications from technology companies and AI researchers delve into recommendation systems. Search for resources from Google AI, Meta, Netflix or reputable platforms like Towards Data Science or Medium.  
* **Books:** 
    * "Programming Collective Intelligence" by Toby Segaran (provides a good conceptual foundation)
    * "Recommender Systems: The Textbook" by  Charu C. Aggarwal (detailed overview and technical depth)
* **Practice with Datasets:** Explore public datasets like the MovieLens dataset, often used for learning recommendation system concepts. This hands-on practice helps you apply theoretical knowledge.

**Understanding Business Applications**

* **Industry Case Studies:** Research how prominent companies in different industries employ recommendation systems. Examples:
    * **E-commerce:**  Amazon's product recommendations
    * **Streaming Services:** Netflix's personalized movie suggestions
    * **Social Media:**  Facebook's and Twitter's content curation
    * **Advertising:** Targeted ad placement based on user profiles
* **Business-focused Articles:** Websites like Harvard Business Review, Forbes, and similar publications regularly feature pieces on leveraging recommendation systems for competitive advantage and optimizing business processes. 
* **Webinars and Workshops:** Explore webinars and online events with a specific focus on the business side of recommendation systems, and network with industry professionals. 

**Areas to Focus On**

* **Customer Experience:** Understand how recommendation systems improve personalization and drive customer satisfaction.
* **Increased Sales and Conversions:** Explore how better recommendations impact product discovery, boosting sales and conversions.
* **Identifying Market Opportunities:** Learn how businesses utilize insights gleaned from recommendation systems to spot new product avenues or trends. 
* **Competitive Advantage:**  Understand the  strategic edge that good recommendations offer.
* **Metrics and Evaluation:**  Familiarize yourself with the metrics used to evaluate recommendation system performance (like accuracy, precision, recall) and their business implications. 

**Tips**

* **Start with the Basics:** Master the fundamentals of collaborative filtering and content-based filtering before exploring more complex techniques.
* **Don't Neglect the Business Lens:** Remember to tie technical knowledge to concrete value  a company can  derive from these systems.
* **Experiment and Build:** Use platforms like Kaggle to find datasets and participate in competitions. Build your own simple recommendation engine to solidify your understanding.
* **Stay Updated:** The field is evolving rapidly. Follow relevant blogs and newsletters to keep abreast of  advancements and new applications.

----

Use MovieLens dataset to write a python program for collaborative filtering and content-based filtering

An example that uses the MovieLens "latest-small" dataset (available at [https://grouplens.org/datasets/movielens/latest/](https://grouplens.org/datasets/movielens/latest/)) to demonstrate both collaborative filtering and content-based filtering.

1. Download and unzip the "ml-latest-small" dataset.
2. Ensure that the files `movies.csv` and `ratings.csv` are accessible by the script (place them in the same directory or provide the correct path).

**Steps:**

- **Collaborative Filtering (User-Based):**
  - We load user ratings from `ratings.csv`.
  - We create a user-item rating matrix.
  - For a given user, we find similar users based on their ratings (e.g., using cosine similarity).
  - We predict unseen movie ratings for that user from similar users’ ratings and recommend the top movies.

- **Content-Based Filtering:**
  - We load movie metadata (titles and genres) from `movies.csv`.
  - Convert genres into a vector representation (multi-hot encoding).
  - Build a user profile from the user’s liked movies by averaging their genre vectors.
  - Recommend new movies by calculating similarity between the user’s profile and all movie vectors.

This is a simplified demonstration. In practice, you might:

- Use more advanced similarity measures (Pearson correlation).
- Normalize user ratings (subtract user mean rating) for collaborative filtering.
- Use TF-IDF or embeddings for more nuanced content representations.

```python
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Load MovieLens small dataset
movies = pd.read_csv('movies.csv')
ratings = pd.read_csv('ratings.csv')

# === Collaborative Filtering (User-Based) ===

# Create a user-item rating matrix
# Rows: userId, Columns: movieId
user_item_matrix = ratings.pivot_table(index='userId', columns='movieId', values='rating', fill_value=0)

# For simplicity, choose a target user (e.g., userId=1)
target_user_id = 1
target_user_ratings = user_item_matrix.loc[target_user_id]

# Compute user-user similarity using cosine similarity
# Convert to array
user_sim_matrix = cosine_similarity(user_item_matrix.values)
user_sim_df = pd.DataFrame(user_sim_matrix, index=user_item_matrix.index, columns=user_item_matrix.index)

# Get similarities for target user
user_sims = user_sim_df.loc[target_user_id]

# Predict ratings for unseen movies
def predict_rating_user_based(target_user_id, movie_id, k=10):
    # Get the top-k similar users who have rated this movie
    if movie_id not in user_item_matrix.columns:
        # Movie not in user-item matrix (no ratings at all)
        return 0
    
    # Similarities of the target user to all other users
    sims = user_sims.drop(target_user_id)
    # Users who rated the movie
    users_who_rated = user_item_matrix[user_item_matrix[movie_id] > 0].index
    # Filter only those users
    sims = sims.loc[sims.index.intersection(users_who_rated)]
    
    if sims.empty:
        # No one rated the movie
        return 0
    
    # Take top-k similar users
    top_k_users = sims.nlargest(k).index
    top_k_sims = sims[top_k_users]
    top_k_ratings = user_item_matrix.loc[top_k_users, movie_id]
    
    # Weighted average
    numerator = np.sum(top_k_sims * top_k_ratings)
    denominator = np.sum(np.abs(top_k_sims)) if np.sum(np.abs(top_k_sims)) != 0 else 1
    return numerator / denominator

# Recommend top movies for the target user (based on predicted rating)
user_seen_movies = ratings[ratings['userId'] == target_user_id]['movieId'].unique()
all_movies = user_item_matrix.columns

predictions = []
for m in all_movies:
    if m not in user_seen_movies:
        pred_rating = predict_rating_user_based(target_user_id, m, k=10)
        predictions.append((m, pred_rating))

# Sort by predicted rating
predictions.sort(key=lambda x: x[1], reverse=True)

print("=== Collaborative Filtering Recommendations for User {} ===".format(target_user_id))
for m, r in predictions[:10]:
    movie_title = movies.loc[movies['movieId'] == m, 'title'].values[0]
    print(f"{movie_title} (predicted rating: {r:.2f})")


# === Content-Based Filtering ===

# Extract genres and create a multi-hot encoding
# Genres are separated by '|'
all_genres = set()
movies['genres'].apply(lambda g: all_genres.update(g.split('|')))
all_genres = list(all_genres)

def encode_genres(genre_str):
    genre_set = set(genre_str.split('|'))
    return [1 if g in genre_set else 0 for g in all_genres]

movies['genre_vector'] = movies['genres'].apply(encode_genres)
genre_matrix = np.vstack(movies['genre_vector'])

# Build a user profile from the target user's liked movies
# For simplicity, assume liked movies are those rated >= 4.0
liked_movies = ratings[(ratings['userId'] == target_user_id) & (ratings['rating'] >= 4.0)]['movieId']
liked_genre_vectors = []
for mid in liked_movies:
    gv = movies.loc[movies['movieId'] == mid, 'genre_vector'].values
    if len(gv) > 0:
        liked_genre_vectors.append(gv[0])

if len(liked_genre_vectors) > 0:
    user_profile = np.mean(liked_genre_vectors, axis=0)
else:
    # If user doesn't have liked movies, fallback to a neutral profile
    user_profile = np.zeros(len(all_genres))

# Compute similarity of all movies to the user profile
sim_to_profile = cosine_similarity([user_profile], genre_matrix)[0]

# Create a dataframe for recommendations
movies['content_similarity'] = sim_to_profile

# Exclude movies the user has already seen
movies_seen = ratings[ratings['userId'] == target_user_id]['movieId'].unique()
cb_recs = movies[~movies['movieId'].isin(movies_seen)].sort_values(by='content_similarity', ascending=False)

print("\n=== Content-Based Recommendations for User {} ===".format(target_user_id))
for i, row in cb_recs.head(10).iterrows():
    print(f"{row['title']} (similarity: {row['content_similarity']:.2f})")
```

**What This Code Does:**

- **Collaborative Filtering:**
  - Builds a user-item matrix from the ratings.
  - Calculates user-user similarity.
  - Predicts ratings for unseen movies for `target_user_id` by a weighted average of the top-k most similar users’ ratings.
  - Prints out top 10 recommended movies.

- **Content-Based Filtering:**
  - Extracts genres from `movies.csv` and creates a multi-hot genre vector for each movie.
  - Builds a user profile from the genres of the movies the user rated highly.
  - Calculates the cosine similarity of each movie’s genre vector to the user’s profile.
  - Recommends top 10 movies not seen by the user, sorted by similarity.

This is a simplified demonstration. Real-world usage would require more careful handling of sparse data, possibly using advanced similarity measures and normalization for collaborative filtering. For content-based, you might use more nuanced features (e.g., TF-IDF of movie descriptions) or other metadata.

----

Use MovieLens dataset to write a python program for Normalize user ratings (subtract user mean rating) for collaborative filtering

An example Python program that demonstrates how to normalize user ratings by subtracting each user’s mean rating before performing collaborative filtering. By normalizing ratings, we often get a better measure of similarity (for example, using Pearson correlation) since this focuses on the user’s preference pattern rather than absolute rating levels.

**Steps:**
1. Load the MovieLens ratings data.
2. Create a user-item rating matrix.
3. Compute each user’s mean rating and subtract it from their ratings to normalize.
4. Compute user-user similarities using Pearson correlation on normalized ratings.
5. Predict unseen ratings by adding back the user’s mean rating after computing a weighted average of similar users’ normalized ratings.

- This example uses the MovieLens "latest-small" dataset.  
- Make sure `ratings.csv` from the MovieLens dataset is in the same directory or adjust the path accordingly.  
- This is a simplified demonstration. In practice, you might apply more robust techniques for similarity and handle sparsity with care.

```python
import pandas as pd
import numpy as np

# Load ratings
ratings = pd.read_csv('ratings.csv')  # Ensure this file is in your working directory

# Create user-item matrix
user_item_matrix = ratings.pivot_table(index='userId', columns='movieId', values='rating')

# Compute user mean ratings
user_means = user_item_matrix.mean(axis=1)

# Normalize by subtracting the user mean from each rating
# This creates a matrix of deviations from each user's mean
user_item_norm = user_item_matrix.sub(user_means, axis=0)

# Replace NaN with 0 for similarity calculations
user_item_norm_filled = user_item_norm.fillna(0)

# Compute user-user similarity using Pearson correlation
# Pearson correlation on the normalized ratings is equivalent to cosine similarity on zero-mean data
user_similarity = np.corrcoef(user_item_norm_filled.values)
user_ids = user_item_norm_filled.index
user_sim_df = pd.DataFrame(user_similarity, index=user_ids, columns=user_ids)

def predict_rating(target_user, target_movie, k=10):
    if target_movie not in user_item_matrix.columns:
        # Movie not found in the dataset
        return user_means.loc[target_user]
    
    # Similarities of target_user to all other users
    sims = user_sim_df.loc[target_user]
    
    # Users who rated target_movie
    users_who_rated = user_item_norm[~user_item_norm[target_movie].isna()].index
    sims = sims.loc[users_who_rated]
    
    # If no one rated the movie, return the user's mean
    if len(sims) == 0:
        return user_means.loc[target_user]
    
    # Sort users by similarity
    top_k_users = sims.nlargest(k).index
    top_k_sims = sims[top_k_users]
    top_k_norm_ratings = user_item_norm.loc[top_k_users, target_movie]
    
    # Weighted sum of normalized ratings
    numerator = np.sum(top_k_sims * top_k_norm_ratings)
    denominator = np.sum(np.abs(top_k_sims)) if np.sum(np.abs(top_k_sims)) != 0 else 1
    
    # Add back the user's mean rating
    return user_means.loc[target_user] + (numerator / denominator)

# Example:
target_user_id = 1
# Pick a movie that user 1 didn't rate
user_1_movies = ratings[ratings['userId'] == target_user_id]['movieId'].unique()
all_movies = user_item_matrix.columns
unseen_movies = [m for m in all_movies if m not in user_1_movies]

# Predict ratings for some unseen movies
print(f"Predictions for User {target_user_id} on unseen movies:")
for m in unseen_movies[:10]:  # just show top 10 for demonstration
    pred = predict_rating(target_user_id, m, k=10)
    print(f"Movie {m}: Predicted rating = {pred:.2f}")
```

**What This Code Does:**

- Loads the user ratings and creates a user-item matrix.
- Computes each user’s average rating.
- Subtracts the mean from each user’s ratings to get normalized ratings.
- Uses Pearson correlation (via `np.corrcoef`) to compute user-user similarity on the normalized ratings.
- Predicts a target user’s rating for a target movie by:
  - Finding the most similar users who rated that movie.
  - Taking a weighted average of their normalized ratings (weighted by similarity).
  - Adding back the target user’s mean rating at the end.

This approach often gives more robust recommendations than using raw ratings, as it focuses on how much a user deviates from their own average rather than their absolute rating scale.

----

Use MovieLens dataset to write a python program for TF-IDF or embeddings for more nuanced content representations

An example of using the MovieLens "latest-small" dataset to construct more nuanced content representations of movies using TF-IDF. In this scenario, we will:

1. Load the `movies.csv` and `tags.csv` files from the MovieLens dataset.
2. Create a textual representation for each movie by combining its title, genres, and any user-provided tags.
3. Use TF-IDF vectorization to convert these textual descriptions into a numeric feature matrix.
4. Compute similarity between movies based on these TF-IDF features.
5. Given a particular movie that a user likes, recommend similar movies using the TF-IDF-based similarity.

**Prerequisites:**
- Download and unzip the "ml-latest-small" dataset from [https://grouplens.org/datasets/movielens/latest/](https://grouplens.org/datasets/movielens/latest/).
- Ensure `movies.csv` and `tags.csv` are in your working directory or adjust paths accordingly.

This is a simplified example. In practice, you might:

- Use more sophisticated text cleaning, handling missing tags.
- Consider applying additional text preprocessing steps (e.g., stopword removal, stemming/lemmatization).
- Incorporate movie descriptions if available (not in the latest-small dataset) or other metadata.

**Code Example:**
```python
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Load movies and tags data
movies = pd.read_csv('movies.csv')  # columns: movieId, title, genres
tags = pd.read_csv('tags.csv')      # columns: userId, movieId, tag, timestamp

# Some preprocessing:
# Combine tags for each movie into a single string
# Group tags by movieId and combine them
tags_agg = tags.groupby('movieId')['tag'].apply(lambda x: " ".join(x)).reset_index()
tags_agg.rename(columns={'tag': 'all_tags'}, inplace=True)

# Merge tags into movies
movies = movies.merge(tags_agg, on='movieId', how='left')

# Fill NaN in tags with empty string
movies['all_tags'] = movies['all_tags'].fillna('')

# Genres are separated by '|', we can just turn them into a space-separated string
movies['genres'] = movies['genres'].str.replace('|', ' ', regex=False)

# Construct a text representation for each movie
# We can combine title, genres, and tags into one text field
movies['text'] = movies['title'] + ' ' + movies['genres'] + ' ' + movies['all_tags']

# Create TF-IDF vectorization
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies['text'])

# Compute similarity between movies (using cosine similarity)
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Let's define a function to get movie recommendations based on a given movie title
def get_recommendations(title, cosine_sim=cosine_sim):
    # Get the index of the movie that matches the title
    idx = movies[movies['title'].str.lower() == title.lower()].index
    if len(idx) == 0:
        print(f"No movie found with title '{title}'")
        return []
    idx = idx[0]
    
    # Get pairwise similarity scores for this movie
    sim_scores = list(enumerate(cosine_sim[idx]))
    # Sort the movies based on similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    # Skip the first since it is the movie itself
    sim_scores = sim_scores[1:11]
    # Get the movie indices
    movie_indices = [i[0] for i in sim_scores]
    return movies.iloc[movie_indices]['title'].values

# Example usage:
movie_title = "Toy Story (1995)"
recommended = get_recommendations(movie_title)
print(f"Movies similar to '{movie_title}':")
for rec in recommended:
    print(rec)
```

**What This Code Does:**

- It reads in the movie and tag information.
- Creates a combined text representation for each movie, including its title, genres, and user tags.
- Uses TF-IDF to turn these textual descriptions into numerical features.
- Computes a similarity matrix between all movies based on these TF-IDF features.
- Provides a function `get_recommendations` that, given a movie title, returns a list of similar movies.

You’ll get a list of movies that share similar textual features (title keywords, genres, tags) to the chosen movie. While the recommendations rely solely on textual features, using TF-IDF rather than simple multi-hot vectors or embeddings leads to more nuanced and potentially more relevant recommendations.


----

stopword removal, stemming/lemmatization

An example of how to perform stopword removal and stemming/lemmatization on text data from a publicly available dataset. We’ll use the `20 Newsgroups` dataset provided by `scikit-learn` as our public dataset. This dataset contains a collection of newsgroup posts categorized into various topics.

**Steps:**

1. Load a subset of the 20 Newsgroups dataset.
2. Choose a document and tokenize it into words.
3. Remove English stopwords using NLTK’s built-in stopword list.
4. Apply stemming using NLTK’s PorterStemmer or apply lemmatization using NLTK’s WordNetLemmatizer.

**Prerequisites:**

- `scikit-learn` installed (`pip install scikit-learn`).
- `nltk` installed (`pip install nltk`).
- Download NLTK data (run `import nltk; nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('omw-1.4')` once).

```python
from sklearn.datasets import fetch_20newsgroups
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import re

# Ensure NLTK stopwords and wordnet are downloaded:
# nltk.download('stopwords')
# nltk.download('wordnet')
# nltk.download('omw-1.4')

# 1. Load a subset of the 20 Newsgroups dataset
newsgroups_train = fetch_20newsgroups(subset='train', categories=['sci.med'], remove=('headers', 'footers', 'quotes'))
documents = newsgroups_train.data

# Let's take one example document
text = documents[0]

# 2. Tokenize the text (simple tokenization by splitting on non-alphanumeric characters)
tokens = [word for word in re.split(r'\W+', text) if word]  # remove empty strings

# 3. Remove stopwords
stop_words = set(stopwords.words('english'))
tokens_no_stop = [w for w in tokens if w.lower() not in stop_words]

# 4a. Stemming using PorterStemmer
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(w) for w in tokens_no_stop]

# 4b. Lemmatization using WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
# Lemmatizer works better with parts of speech tags, but here we use the default (noun)
lemmatized_tokens = [lemmatizer.lemmatize(w.lower()) for w in tokens_no_stop]

print("Original Text (first 200 chars):")
print(text[:200], "...\n")

print("Tokens (first 20):", tokens[:20])
print("Tokens after stopword removal (first 20):", tokens_no_stop[:20])
print("Stemmed tokens (first 20):", stemmed_tokens[:20])
print("Lemmatized tokens (first 20):", lemmatized_tokens[:20])
```

**What the Code Does:**

- Loads only the `sci.med` category from the 20 Newsgroups dataset for simplicity.
- Takes the first document and tokenizes it by splitting on non-alphanumeric characters.
- Removes English stopwords such as “the”, “is”, “and”, etc.
- Applies stemming, which reduces words to their stem forms (e.g., “studying” → “study”).
- Applies lemmatization, which reduces words to their dictionary forms (e.g., “studies” → “study”), providing a more linguistically informed approach than stemming.

**Comparison of Stemming and Lemmatization:**

- Stemming is a rule-based approach that often chops words without considering context, resulting in stem forms that may not be actual words.
- Lemmatization uses dictionaries and linguistic knowledge to return words to their lemmas, usually producing more accurate and readable root forms.

This approach is commonly used in text preprocessing steps prior to applying machine learning or NLP models, helping normalize the text and reduce noise.
