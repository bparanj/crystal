Quantifying the degree of uncertainty in linear regression is crucial because it tells you how much confidence you can have in your model's predictions. Let's explore this in simple terms.

### What is Uncertainty in Linear Regression?

When you use linear regression to predict outcomes based on certain inputs, your predictions are not going to be perfect. This imperfection or unpredictability is what we refer to as "uncertainty." It's important to quantify this uncertainty to understand how reliable your model's predictions are.

### Types of Uncertainty

1. **Model Uncertainty**: This reflects the uncertainty in the model parameters themselves—the slope and intercept of the line in linear regression. It's about how well the model fits the data.

2. **Predictive Uncertainty**: This is about the uncertainty in making predictions for new data. Even if your model fits past data well, new predictions can still be uncertain due to factors like data noise or variability not captured by the model.

### How to Quantify Uncertainty

1. **Confidence Intervals**: For model parameters (like the slope and intercept), you can calculate confidence intervals. A confidence interval provides a range within which the true parameter value is likely to fall. For example, if the slope of your regression line has a confidence interval of 2 ± 0.5, it means we're fairly confident that the true slope is between 1.5 and 2.5.

2. **Prediction Intervals**: While confidence intervals give you a range for the model parameters, prediction intervals give you a range for future predictions. A prediction interval accounts for the uncertainty in a single prediction and is usually wider than the confidence interval, reflecting both the uncertainty in estimating the model parameters and the inherent data variability.

### Calculating Uncertainty

- **Statistical Software**: Calculating confidence and prediction intervals usually involves complex mathematical formulas. Statistical software and programming libraries (like R, Python's `scikit-learn`, or `statsmodels`) can compute these automatically.

### Why It Matters

Understanding and quantifying uncertainty in your predictions is crucial for making informed decisions. If the uncertainty is high, you might need more data, a different model, or to accept that some outcomes are inherently unpredictable.

### Key Takeaways

- **Uncertainty in linear regression** refers to the unpredictability in the model's parameters and its predictions.
- **Confidence intervals** help quantify the uncertainty in the model parameters, while **prediction intervals** provide a range for how much  predictions might vary.
- Utilizing statistical software can simplify the process of calculating these intervals, giving you a clearer picture of your model's reliability.

Appreciating the role of uncertainty in predictive modeling enhances your ability to develop and interpret models responsibly, ensuring that the decisions made based on your models are well-informed and consider the inherent risks.

----

Write a Python program to demonstrate Confidence Intervals

Python program that demonstrates how to construct confidence intervals for the mean of a dataset.

1. Generate a synthetic dataset (e.g., from a normal distribution).
2. Compute the sample mean and sample standard deviation.
3. Construct a confidence interval for the mean assuming the population is normally distributed or that the sample is large enough for the Central Limit Theorem to apply.
4. As an alternative, show how to use a bootstrap approach to create a non-parametric confidence interval.

**Steps:**

- Import necessary libraries (NumPy, SciPy, and optionally matplotlib).
- Generate synthetic data.
- Compute the 95% confidence interval for the mean using a parametric approach (assuming normality).
- Compute the 95% bootstrap confidence interval for the mean.

**Code:**

```python
import numpy as np
from scipy.stats import t
import matplotlib.pyplot as plt

# 1. Generate synthetic data
# Let's assume we have a sample from a normal distribution with unknown mean and variance.
np.random.seed(42)  # for reproducibility
data = np.random.normal(loc=5.0, scale=2.0, size=100)  # mean=5, std=2, n=100

# 2. Compute sample statistics
sample_mean = np.mean(data)
sample_std = np.std(data, ddof=1)  # unbiased estimator with ddof=1
n = len(data)

# 3. Construct a 95% confidence interval for the mean (parametric approach)
# Since we know nothing about the population variance a priori, we use the t-distribution.
confidence = 0.95
alpha = 1 - confidence
# Degrees of freedom = n - 1
df = n - 1
# t-critical value for two-tailed test
t_crit = t.ppf(1 - alpha/2, df)

# Margin of error
margin_of_error = t_crit * (sample_std / np.sqrt(n))
ci_lower = sample_mean - margin_of_error
ci_upper = sample_mean + margin_of_error

print(f"Parametric 95% CI for the mean: ({ci_lower:.3f}, {ci_upper:.3f})")

# 4. Bootstrap confidence interval (non-parametric)
# Resample from the data with replacement many times and compute the mean each time.
bootstrap_reps = 10_000
bootstrap_means = []
for _ in range(bootstrap_reps):
    sample = np.random.choice(data, size=n, replace=True)
    bootstrap_means.append(np.mean(sample))

bootstrap_means = np.array(bootstrap_means)
# 95% CI from bootstrap: take the 2.5th and 97.5th percentiles
ci_lower_boot = np.percentile(bootstrap_means, 2.5)
ci_upper_boot = np.percentile(bootstrap_means, 97.5)

print(f"Bootstrap 95% CI for the mean: ({ci_lower_boot:.3f}, {ci_upper_boot:.3f})")

# Optional: Visualize bootstrap distribution
plt.figure(figsize=(8,5))
plt.hist(bootstrap_means, bins=30, alpha=0.7, color='blue', edgecolor='black')
plt.axvline(ci_lower_boot, color='red', linestyle='--', label='95% CI lower (bootstrap)')
plt.axvline(ci_upper_boot, color='red', linestyle='--', label='95% CI upper (bootstrap)')
plt.axvline(sample_mean, color='green', label='Sample Mean')
plt.title('Bootstrap Distribution of Sample Means')
plt.xlabel('Mean')
plt.ylabel('Frequency')
plt.legend()
plt.show()
```

**What This Code Does:**

- Generates a synthetic dataset from a normal distribution.
- Calculates a parametric confidence interval for the mean using the t-distribution.
- Uses bootstrapping (resampling the dataset many times) to construct a non-parametric confidence interval.
- Prints both intervals and provides a histogram of the bootstrap mean distribution with the CI and mean marked.

**Adjustments:**

- Change the confidence level by adjusting `confidence`.
- Experiment with larger or smaller samples, different distributions, or more bootstrap iterations.
- Apply these methods to real-world data instead of synthetic data.

----

Create a video for  Confidence Intervals


Python code example that creates a video to demonstrate how confidence intervals work.

1. **Conceptual Scenario:**  
   We have a population with a known true mean (μ) and known standard deviation (σ). We repeatedly draw samples from this population, compute the sample mean, and construct a 95% confidence interval for the mean based on the t-distribution.  
   
2. **Animation Idea:**  
   - On each frame (iteration), draw a new sample from the population.
   - Compute the sample mean and a 95% confidence interval for the mean.
   - Plot the confidence interval as a line segment on a graph where the horizontal axis is the iteration number and the vertical axis is the mean.  
   
   Over multiple frames, you will see many intervals. About 95% of them should contain the true mean (drawn as a horizontal line), and about 5% should not. This visually demonstrates the interpretation of confidence intervals (i.e., in the long run, 95% of the constructed intervals from identical experiments cover the true mean).

**Note:**  

- This animation does not produce an interactive explanation of formula derivation, but it visually shows the frequentist interpretation of confidence intervals.
- Requires `ffmpeg` to save the animation as an MP4 file.
- Adjust the number of iterations (frames) for a longer animation.

**Code Example (run locally):**

```python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from scipy.stats import t

# Population parameters
true_mean = 5.0
true_std = 2.0
n = 30           # sample size per iteration
confidence = 0.95
alpha = 1 - confidence
df = n - 1

# t-critical value for 95% CI
t_crit = t.ppf(1 - alpha/2, df)

# Number of intervals to show
iterations = 50

# We will store intervals
intervals = []   # each element: (lower, upper, mean)
contains_mean = []

def generate_interval():
    # Draw a sample from the population
    sample = np.random.normal(loc=true_mean, scale=true_std, size=n)
    sample_mean = np.mean(sample)
    sample_std = np.std(sample, ddof=1)
    margin_of_error = t_crit * (sample_std / np.sqrt(n))
    lower = sample_mean - margin_of_error
    upper = sample_mean + margin_of_error
    return (lower, upper, sample_mean, (lower <= true_mean <= upper))

# Pre-generate all intervals
for _ in range(iterations):
    interval = generate_interval()
    intervals.append(interval)

fig, ax = plt.subplots(figsize=(8,6))
ax.axhline(true_mean, color='green', linewidth=2, label='True Mean')

# We'll show intervals on the x-axis from 1 to iterations
ax.set_xlim(0.5, iterations+0.5)
ax.set_ylim(true_mean - 3*true_std, true_mean + 3*true_std)
ax.set_xlabel('Iteration')
ax.set_ylabel('Mean and 95% CI')
ax.set_title('Demonstration of 95% Confidence Intervals')

lines = []
points = []

for i in range(iterations):
    line, = ax.plot([], [], color='red', linewidth=2)
    point = ax.plot([], [], 'ro')[0]
    lines.append(line)
    points.append(point)

text_counter = ax.text(0.5, 1.02, "", transform=ax.transAxes, ha='center', va='center', fontsize=12)

ax.legend(loc='upper right')

def init():
    for line in lines:
        line.set_data([], [])
    for p in points:
        p.set_data([], [])
    text_counter.set_text("")
    return lines + points + [text_counter]

def update(frame):
    # frame: 0 to iterations-1
    lower, upper, m, contained = intervals[frame]
    x_pos = frame + 1
    # Update the line for this interval
    lines[frame].set_data([x_pos, x_pos], [lower, upper])
    # Update the point for the sample mean
    points[frame].set_data(x_pos, m)
    if contained:
        lines[frame].set_color('blue')
        points[frame].set_color('blue')
    else:
        lines[frame].set_color('red')
        points[frame].set_color('red')

    # Count how many intervals contain the mean so far
    count_contain = sum(i[3] for i in intervals[:frame+1])
    prop = count_contain/(frame+1)

    text_counter.set_text(f"Iteration {frame+1}: {count_contain}/{frame+1} intervals contain true mean ({prop:.1%})")

    return lines + points + [text_counter]

anim = animation.FuncAnimation(fig, update, frames=iterations, init_func=init, blit=False, repeat=False)

# Save the animation as MP4
anim.save('confidence_intervals_demo.mp4', writer='ffmpeg', fps=2)

plt.show()
```

**What This Code Does:**

- Simulates repeated sampling from a known population.
- For each sample, it calculates a 95% CI for the mean.
- Animates the accumulation of these intervals, showing which contain the true mean and which do not.
- Displays a running count and proportion of intervals that contain the true mean, illustrating the frequentist interpretation: about 95% of these intervals should contain the true mean in the long run.

**Adjustments:**

- Change `iterations` to generate a longer or shorter animation.
- Adjust the confidence level, sample size `n`, or population parameters.
- Use different distributions or a non-parametric interval.

After running the code locally, open `confidence_intervals_demo.mp4` to view the conceptual animation of confidence intervals and how they behave over multiple sampling experiments.

----

Write a Python program for Prediction Intervals

Python program that demonstrates how to compute prediction intervals for a simple linear regression model.

Prediction intervals differ from confidence intervals for the mean response in that they provide an interval estimate for an individual new observation rather than the average response. Prediction intervals are wider because they incorporate not only the uncertainty in estimating the mean response but also the variability of individual new observations.

**Steps:**

1. Generate synthetic data from a known linear model with noise.
2. Fit a simple linear regression model to the data.
3. Compute the residual standard error.
4. For a given new predictor value \( x_0 \), compute the standard error of the predicted value.
5. Calculate the prediction interval using the t-distribution quantile.

**Formula for the Prediction Interval:**

For a simple linear regression \( y = \beta_0 + \beta_1 x + \epsilon \) fitted on n data points, an estimate for a new observation at \( x_0 \) is:
\[
\hat{y}(x_0) = \hat{\beta}_0 + \hat{\beta}_1 x_0
\]

The standard error for the prediction at \( x_0 \) is:
\[
\text{SE}_{pred}(x_0) = s \sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{\sum (x_i - \bar{x})^2}}
\]
where:
- \( s \) is the residual standard error,
- \( \bar{x} \) is the mean of the observed \( x \)-values.

A \( (1-\alpha) \)-prediction interval is:
\[
\hat{y}(x_0) \pm t_{\alpha/2, n-2} \times \text{SE}_{pred}(x_0)
\]

**Code:**

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

# Set seed for reproducibility
np.random.seed(42)

# 1. Generate synthetic linear data with noise
n = 30
x = np.linspace(0, 10, n)
true_beta0 = 2.0
true_beta1 = 0.5
true_sigma = 2.0  # standard deviation of noise
y = true_beta0 + true_beta1 * x + np.random.normal(0, true_sigma, n)

# Fit a linear regression model: y = b0 + b1*x
X = np.vstack([np.ones(n), x]).T
# (X^T X)^{-1} X^T y
beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
b0, b1 = beta_hat

# Compute residuals and residual standard error
y_hat = b0 + b1 * x
residuals = y - y_hat
df = n - 2  # degrees of freedom for simple linear regression
s = np.sqrt(np.sum(residuals**2) / df)  # residual standard error

# Choose a new x0 to predict at
x0 = 5.0
y0_hat = b0 + b1 * x0

# Compute SE_pred(x0)
x_mean = np.mean(x)
SSX = np.sum((x - x_mean)**2)
SE_pred = s * np.sqrt(1 + 1/n + ((x0 - x_mean)**2 / SSX))

# t-critical value for 95% prediction interval
confidence = 0.95
alpha = 1 - confidence
t_crit = t.ppf(1 - alpha/2, df)

margin_of_error = t_crit * SE_pred
pred_interval = (y0_hat - margin_of_error, y0_hat + margin_of_error)

print(f"Predicted value at x0={x0:.2f}: {y0_hat:.2f}")
print(f"95% Prediction Interval: {pred_interval}")

# Optional: Visualize the regression line and the prediction interval for a range of x-values
x_new = np.linspace(0, 10, 100)
y_pred = b0 + b1 * x_new

# Compute prediction intervals for all x_new
SE_pred_all = s * np.sqrt(1 + 1/n + ((x_new - x_mean)**2 / SSX))
margin_all = t_crit * SE_pred_all
lower_PI = y_pred - margin_all
upper_PI = y_pred + margin_all

plt.figure(figsize=(8,6))
plt.scatter(x, y, label='Data', color='blue')
plt.plot(x_new, y_pred, 'r-', label='Regression Line')
plt.plot(x_new, lower_PI, 'r--', label='95% Prediction Interval')
plt.plot(x_new, upper_PI, 'r--')
plt.scatter(x0, y0_hat, color='green', label='Prediction at x0')
plt.vlines(x0, pred_interval[0], pred_interval[1], color='green', linestyle='--')
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.title('Linear Regression with 95% Prediction Intervals')
plt.show()
```

**What This Code Does:**

- Generates a dataset from a known linear relationship with noise.
- Fits a linear model and computes the slope and intercept.
- Calculates the residual standard error and uses it to determine the 95% prediction interval for a new observation at \( x_0 \).
- Visualizes the regression line and the prediction intervals around it.

**Adjustments:**

- Change the sample size `n`, noise level, or polynomial form.
- Use `scikit-learn` for regression if desired.
- Apply to real data and interpret the prediction intervals accordingly.


----

Create a video for Prediction Intervals

Python code that creates a video to demonstrate how prediction intervals behave in a linear regression scenario.

1. **Scenario:**  
   Consider a linear regression line fit to some data. We know each new observation \( y_{new} \) at a given \( x_0 \) is not just on the line, but is subject to noise. Prediction intervals give a range where we expect a *new individual observation* to fall, rather than just the mean response.

2. **Animation:**  
   - Start with some data points and a fitted regression line.
   - At each frame, pick a new \( x_0 \) along the x-axis and draw a vertical line showing the predicted value plus a 95% prediction interval.
   - Plot simulated “new observations” from the underlying model at that \( x_0 \) and show how they often, but not always, fall within the predicted interval.
   - Over the animation, you see how the prediction interval changes as \( x_0 \) moves, and how new points at various \( x_0 \) values land in or outside the intervals.

- This example uses a static fitted line and noise model. The intervals are computed once from the model and then displayed for different \( x_0 \) values.
- Requires `ffmpeg` to save the animation as MP4.

**Code Example (run locally):**

```python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from scipy.stats import t

np.random.seed(42)

# Generate synthetic linear data
n = 30
x = np.linspace(0, 10, n)
true_beta0 = 2.0
true_beta1 = 0.5
true_sigma = 2.0
y = true_beta0 + true_beta1 * x + np.random.normal(0, true_sigma, n)

# Fit linear model: y = b0 + b1*x
X = np.vstack([np.ones(n), x]).T
beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
b0, b1 = beta_hat

# Residual standard error
y_hat = b0 + b1*x
residuals = y - y_hat
df = n-2
s = np.sqrt(np.sum(residuals**2)/df)

# Compute critical t-value for 95% intervals
confidence = 0.95
alpha = 1 - confidence
t_crit = t.ppf(1 - alpha/2, df)

x_mean = np.mean(x)
SSX = np.sum((x - x_mean)**2)

def prediction_interval(x0):
    # Prediction interval for a new observation at x0
    y0_hat = b0 + b1*x0
    SE_pred = s * np.sqrt(1 + 1/n + ((x0 - x_mean)**2 / SSX))
    margin = t_crit * SE_pred
    return y0_hat - margin, y0_hat + margin, y0_hat

# Set up animation
fig, ax = plt.subplots(figsize=(8,6))
ax.scatter(x, y, c='blue', label='Data')
ax.plot(x, y_hat, 'r-', label='Regression Line')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Prediction Intervals for New Observations')
ax.legend()

# We'll pick a sequence of x0 values across the range and show intervals
x0_values = np.linspace(0, 10, 30)
new_points_x = []
new_points_y = []
interval_lines = []
point_scatters = []

interval_line, = ax.plot([], [], 'g--', lw=2)
point_scatter = ax.scatter([], [], c='green', alpha=0.7, s=30)

def init():
    interval_line.set_data([], [])
    point_scatter.set_offsets([])
    return [interval_line, point_scatter]

def update(frame):
    x0 = x0_values[frame]
    lower, upper, pred = prediction_interval(x0)
    # Draw the vertical line for the interval
    interval_line.set_data([x0, x0], [lower, upper])

    # Simulate some new observations at x0
    # We'll simulate multiple points each frame to see distribution
    new_y = true_beta0 + true_beta1*x0 + np.random.normal(0, true_sigma, 10)
    offsets = np.column_stack((np.full(10, x0), new_y))
    point_scatter.set_offsets(offsets)
    point_scatter.set_color(['green' if (ny >= lower and ny <= upper) else 'red' for ny in new_y])

    return [interval_line, point_scatter]

anim = animation.FuncAnimation(fig, update, frames=len(x0_values),
                               init_func=init, blit=False, repeat=False)

# Save animation as MP4
anim.save('prediction_intervals_demo.mp4', writer='ffmpeg', fps=2)

plt.show()
```

**What This Code Does:**

- Fits a simple linear regression to synthetic data.
- For each frame in the animation:
  - Selects a new \( x_0 \).
  - Computes the 95% prediction interval at that \( x_0 \).
  - Simulates new observations at \( x_0 \) and plots them, coloring green if inside the interval and red if outside.
- Over the animation, you see the interval and various simulated points, illustrating how a prediction interval captures where a new observation is likely to appear.

**Adjustments:**

- Change the number of simulated points per frame, the noise level, or sample size.
- Use different confidence levels.
- Implement a scenario where the line is not well-fit to see wider intervals.

After running the code locally, open `prediction_intervals_demo.mp4` to view the animation of prediction intervals and their interpretation for new observations.
