Evaluating the quality of clusters obtained

Evaluating cluster quality is a crucial step, as clustering algorithms don't magically output 'correct' answers.

**Types of Evaluation**

* **Internal Evaluation:** Focuses on measures calculated from the data and clustering structure itself, without using external information like pre-existing labels.
* **External Evaluation:** When you *do* have true labels to compare against, these metrics tell you how well the clustering aligns with those known groups.
* **Relative Evaluation:** Comparing outputs from different clustering algorithms or parameter choices using internal evaluation metrics to select the best fit for your data.

**Evaluation Measures**

**Internal Metrics**

* **Silhouette Coefficient:**  Calculated for each point. Measures how similar it is to its own cluster vs. the nearest neighboring cluster. Average this across all points. Good scores (closer to 1) indicate well-defined clusters.
* **Dunn Index:** Ratio of between-cluster separation to within-cluster spread. Higher values signal better-separated, more compact clusters.
* **Elbow Method (for K-means):** Track the sum of squared distances (inertia) between samples and their centroids, as you incrementally increase the number of clusters. Look for an 'elbow' where adding more clusters provides drastically less improvement.

**External Metrics** 

* **Purity:** For a cluster, what fraction of samples belong to its dominant true label? High purity is good.
* **Rand Index, Adjusted Rand Index:** Assess overall agreement between known labels and clustering assignment. Closer to 1 suggests better alignment. 
* **Homogeneity and Completeness:**  Complementary scores. Homogeneity means a cluster  mainly contains a single  class, completeness implies all members of a given class are assigned to the same cluster. You ideally want both!

**The Human Factor – Never Neglect Visualizations and Domain Knowledge**

* **Plots:** If available, visualizing clusters in 2D/3D with color-coded assignments often brings qualitative insights beyond simple metrics. Look for well separated groups or suspect overlaps.   
* **Business Context:** Do the clusters make sense given what you know about your domain? For example, would customer groupings in retail data align with your understanding of their purchasing behaviors?

**Considerations**

* **No Universal 'Best' Metric:** What matters depends on your dataset and objective. A cluster structure good for one goal might be unsuitable for another.
* **Beware High Scores in a Vacuum:** Extremely compact clusters could exist even in random data. If the segmentation holds no practical meaning, high performance on metrics can be deceiving.

* Let's calculate these metrics in Python (Scikit-learn) or R and interpret them.
* **Have a clustering output on a dataset:** We can assess its quality *without* true labels just by examining the internal measurements and visualization.
* **Discuss situations:** Where can different metrics guide a decision between multiple possible clustering solutions? 


Evaluate cluster quality using metrics such as Silhouette Coefficient, Dunn Index and Elbow Method (for K-means) in Python (Scikit-learn) and interpret them.

## Implementation

Python example using the Iris dataset and K-Means clustering to evaluate cluster quality using the Silhouette Coefficient, Dunn Index, and the Elbow Method. 

1. **Load and preprocess the data**: We use the Iris dataset and scale it.
2. **Apply K-Means with different numbers of clusters (k)**: For each k, we:
   - Compute cluster labels.
   - Calculate the Silhouette Score using `silhouette_score` (scikit-learn).
   - Calculate the Dunn Index (not in scikit-learn by default, so we define a function).
   - Calculate the Within-Cluster Sum of Squares (WCSS) for the Elbow Method.
3. **Interpretation**:
   - **Silhouette Score**: Measures how well-separated and cohesive the clusters are. Higher is better (max is 1).
   - **Dunn Index**: Ratio of minimum inter-cluster distance to the maximum intra-cluster diameter. Higher is better, indicating well-separated and compact clusters.
   - **Elbow Method (WCSS)**: Look for a point where WCSS decreases slower after a certain k. That “elbow” suggests a reasonable number of clusters.

**Code Example:**

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.spatial.distance import cdist

# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target

# Scale features
X_scaled = StandardScaler().fit_transform(X)

def dunn_index(X, labels):
    # Compute Dunn Index
    # Steps:
    # 1. Compute cluster centroids and form clusters.
    # 2. Compute inter-cluster distances (distance between cluster centroids).
    # 3. Compute intra-cluster diameters (max distance between points in the same cluster).
    # 4. Dunn = min(inter-cluster distance) / max(intra-cluster diameter)

    unique_labels = np.unique(labels)
    clusters = [X[labels == lbl] for lbl in unique_labels]

    # Intra-cluster diameters: max distance between any two points in the cluster
    intra_diameters = []
    for cluster in clusters:
        if len(cluster) > 1:
            distances = cdist(cluster, cluster)
            intra_diameters.append(distances.max())
        else:
            # If a cluster has one point, diameter is 0
            intra_diameters.append(0.0)

    # Inter-cluster distances: min distance between any two clusters
    # We'll measure distance between centroids or use min pairwise inter-cluster distance
    # For a stricter measure, use min distance between points of different clusters
    inter_distances = []
    for i in range(len(clusters)):
        for j in range(i+1, len(clusters)):
            distances = cdist(clusters[i], clusters[j])
            inter_distances.append(distances.min())

    if len(inter_distances) == 0:
        # If there's only one cluster, Dunn index is not defined
        return np.nan

    dunn = min(inter_distances) / max(intra_diameters)
    return dunn

K_values = range(2, 11)
sil_scores = []
dunn_scores = []
wcss_values = []

for k in K_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    labels = kmeans.labels_

    # Silhouette Score
    sil = silhouette_score(X_scaled, labels)
    sil_scores.append(sil)

    # Dunn Index
    dunn = dunn_index(X_scaled, labels)
    dunn_scores.append(dunn)

    # WCSS (inertia_)
    wcss = kmeans.inertia_
    wcss_values.append(wcss)

# Print Results
results_df = pd.DataFrame({
    'k': K_values,
    'Silhouette Score': sil_scores,
    'Dunn Index': dunn_scores,
    'WCSS': wcss_values
})

print(results_df)

# Interpretation:
# 1. Silhouette Score:
#    Look for the number of clusters that gives a relatively high score.
# 2. Dunn Index:
#    Higher values suggest better separation and compactness.
# 3. Elbow Method (WCSS):
#    Plot k vs WCSS and look for a bend in the curve.

# Optional: Plotting for better visualization
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 3, figsize=(15, 4))

# Silhouette Scores
axs[0].plot(K_values, sil_scores, marker='o')
axs[0].set_title("Silhouette Score vs k")
axs[0].set_xlabel("Number of clusters (k)")
axs[0].set_ylabel("Silhouette Score")

# Dunn Index
axs[1].plot(K_values, dunn_scores, marker='o')
axs[1].set_title("Dunn Index vs k")
axs[1].set_xlabel("Number of clusters (k)")
axs[1].set_ylabel("Dunn Index")

# WCSS
axs[2].plot(K_values, wcss_values, marker='o')
axs[2].set_title("WCSS (Elbow Method) vs k")
axs[2].set_xlabel("Number of clusters (k)")
axs[2].set_ylabel("WCSS")

plt.tight_layout()
plt.show()
```

**How to Interpret These Results:**

- **Silhouette Score**: A higher value (closer to 1) means well-separated clusters. Values around 0 indicate overlapping clusters, and negative values indicate incorrect clustering.
  
- **Dunn Index**: A higher value indicates that clusters are more separated and compact. A drop in the Dunn Index as you increase k might indicate that clusters are getting worse.

- **Elbow Method (WCSS)**: Plot `k` vs `WCSS`. Initially, WCSS decreases sharply as k increases. Eventually, the rate of decrease slows down, forming an “elbow.” The k at the elbow is often a good trade-off between simplicity and cluster quality.

Using all three can help cross-validate the choice of k:

- The elbow from WCSS guides you roughly.
- The Silhouette Score can confirm if that k gives good separation.
- The Dunn Index provides another perspective on cluster compactness and separation.

If all three metrics suggest a similar number of clusters, you can be more confident in that choice.
