Clustering the data through K-means & DBSCAN Clustering

K-means and DBSCAN are clustering methods. They find underlying structure in data.

**K-means Clustering**

1. Aims to partition data into 'K' distinct clusters, where you predetermine the value of 'K'.
2. **How it Works:**
   * Starts by randomly placing 'K' centroids (center points) in your data space.
   * Assigns each data point to its nearest centroid.
   * Recalculates centroid positions as the average of points in a cluster.
   * Repeats assignment and recalculation until centroids stabilize (or a max iteration count is  reached).
3. **Strengths:**
   * Fast and scales well for large datasets.
   * Reasonably good with compact, spherical clusters.  
4. **Weaknesses:**
   * You need to choose 'K' in advance, which can be tricky if you don't know the natural number of groups.
   * Sensitive to the initial placement of centroids (different starting points could lead to different outcomes).
   * Assumes clusters are similarly sized and shaped, struggling with uneven data distributions.

**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**

1. Finds areas of high density separated by sparse regions. Doesn't focus on forcing all data points into clusters. 
2. **How it Works**
    * Two key parameters:  
        * eps: Neighborhood radius around a data point.
        * minPts: Minimum number of points needed within the eps radius to be considered a 'core' point.
    * Core points form the 'seeds' of clusters. Points near them get added to the same cluster.
    * Noise points: Ones neither core points nor near any core points.
3. **Strengths:**
    * Doesn't need to know the number of clusters upfront.
    * Good with clusters of arbitrary shapes (non-spherical).
    * Robust to outliers, classifying them as noise.
4. **Weaknesses:**    
    * Parameter tuning (eps, minPts) can be  tricky and heavily depend on your data's density. 
    * Might struggle with datasets having varying densities in different regions. 

**When to Choose Which**

* **Well-Defined, Even Clusters:** If you expect relatively clear cluster separation and know (or have a hunch) about the number of clusters, K-means is a good starting point.
* **Unclear Cluster Count, Complex Shapes:** If you suspect odd cluster shapes, and don't want to make assumptions about the ideal number of clusters, DBSCAN is worth exploring.
* **Don't Forget Outliers:** Datasets having many outliers and noise benefit from DBSCAN's built-in handling of them.

**Considerations**

* **Feature scaling:**  Before clustering, normalize or standardize features with varying scales, as distance calculations are sensitive to this.
* **Visualization** Always plot clustered data to visually check if the results make sense.
* **No 'One Size Fits All':** Experimentation is necessary to find the right clustering algorithm and parameters for your specific dataset.

**Practice**

* **Code walkthrough** of both methods in Python/R.
* **Use a publicly available dataset** (e.g., customer spending data) and see how K-means and DBSCAN give different findings
* **Discuss advanced topics** like hierarchical clustering or choosing cluster count with K-means (Elbow Method).

## Customer Spending

Use a publicly available dataset such as customer spending data and see how K-means and DBSCAN clustering give different findings using Python.

An example using the "Mall Customer Segmentation" dataset, which is publicly available (e.g. from Kaggle). This dataset includes customer IDs, gender, age, annual income, and spending score. We will compare K-Means and DBSCAN clustering results on the annual income and spending score features.

**Steps:**

1. Load and preprocess the data.
2. Apply K-Means clustering.
3. Apply DBSCAN clustering.
4. Visualize and compare results.

**Code:**

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler

# Load dataset (assumes "Mall_Customers.csv" is in the working directory)
# The dataset is available here: https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python
df = pd.read_csv('Mall_Customers.csv')

# Select features: Annual Income (k$) and Spending Score
X = df[['Annual Income (k$)', 'Spending Score (1-100)']]

# Standardize the data for better clustering performance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means with a chosen number of clusters (e.g., 5)
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans_labels = kmeans.fit_predict(X_scaled)

# Apply DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)  # parameters may need tuning
dbscan_labels = dbscan.fit_predict(X_scaled)

# Plot K-Means clusters
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans_labels, cmap='Set1')
plt.title('K-Means Clustering')
plt.xlabel('Annual Income (standardized)')
plt.ylabel('Spending Score (standardized)')

# Plot DBSCAN clusters
plt.subplot(1, 2, 2)
# DBSCAN may assign -1 for noise points
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=dbscan_labels, cmap='Set1')
plt.title('DBSCAN Clustering')
plt.xlabel('Annual Income (standardized)')
plt.ylabel('Spending Score (standardized)')

plt.tight_layout()
plt.show()
```

**Insights:**

- K-Means forces data into a specified number of clusters, resulting in a more uniform, spherical partition.  
- DBSCAN finds clusters of dense points and labels outliers as noise. It may discover non-spherical clusters and can identify points that don’t fit well into any group.
- The difference in cluster shapes and the presence of noise points often leads to unique insights. K-Means might give a neat segmentation, while DBSCAN might uncover more nuanced patterns or show where data points do not belong to any well-defined cluster.

## Implementation

An outline and a Python code example that demonstrates K-Means and DBSCAN clustering through an animation (video).

1. Generate a dataset that is not well-suited for K-Means’ assumption of spherical clusters. The “moons” dataset from `sklearn.datasets` is a good choice.
2. Animate the K-Means clustering process:
   - Initialize K-Means with a chosen number of clusters.
   - Run multiple iterations of K-Means, at each iteration:
     - Update cluster centers.
     - Assign points to the nearest cluster.
     - Plot the clustering result at each step.
3. After finishing K-Means iterations, show the final result of DBSCAN clustering on the same data. DBSCAN can identify more complex shapes and might cluster the moons more naturally.
4. Save the animation as a video file (e.g., MP4).

**Steps:**

- Use `make_moons` to create a dataset with two interleaving half-circles.
- Run K-Means for a fixed number of iterations, capturing the changing cluster assignments at each iteration.
- Then run DBSCAN once and show its result in the final frame.
- Use `matplotlib.animation.FuncAnimation` to create the animation frames.
- Export the animation to an MP4 file (requires `ffmpeg` or another suitable writer installed).

**Code Example (run locally):**

```python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from sklearn.cluster import KMeans, DBSCAN
from sklearn.datasets import make_moons

# Generate a dataset
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)

# K-Means setup
k = 2
kmeans = KMeans(n_clusters=k, init='random', n_init=1, max_iter=1, random_state=42)
# We'll manually perform multiple iterations to show the animation step by step.
# Initialize cluster centers by running one iteration
kmeans.fit(X)
centers = kmeans.cluster_centers_
labels = kmeans.labels_

# Parameters for animation
n_iterations = 10  # total iterations to show for K-Means

# Prepare DBSCAN (we'll run it after K-Means animation)
dbscan = DBSCAN(eps=0.2, min_samples=5)
dbscan_labels = dbscan.fit_predict(X)

fig, ax = plt.subplots(figsize=(8, 6))
sc = ax.scatter(X[:,0], X[:,1], c=labels, cmap='Set1', s=50, alpha=0.7)
centers_sc = ax.scatter(centers[:,0], centers[:,1], c='black', s=100, marker='X', edgecolors='white')
ax.set_title('K-Means Clustering Iterations', fontsize=14)

def init():
    # Initial frame: show initial K-Means result
    sc.set_offsets(X)
    sc.set_array(labels)
    centers_sc.set_offsets(centers)
    return sc, centers_sc

def update(frame):
    global kmeans, centers, labels
    if frame < n_iterations:
        # Perform another iteration of K-Means
        kmeans = KMeans(n_clusters=k, init=centers, n_init=1, max_iter=1, random_state=42)
        kmeans.fit(X)
        labels = kmeans.labels_
        centers = kmeans.cluster_centers_

        sc.set_array(labels)
        centers_sc.set_offsets(centers)
        ax.set_title(f'K-Means Iteration {frame+1} of {n_iterations}', fontsize=14)
        return sc, centers_sc
    else:
        # After finishing K-Means iterations, show DBSCAN result
        sc.set_array(dbscan_labels)
        centers_sc.set_offsets(np.empty((0,2)))  # remove K-Means centers
        ax.set_title('DBSCAN Result', fontsize=14)
        return sc, centers_sc

anim = animation.FuncAnimation(fig, update, frames=n_iterations+1,
                               init_func=init, blit=False, repeat=False)

# Save animation as MP4 (requires ffmpeg)
anim.save('kmeans_dbscan_animation.mp4', writer='ffmpeg', fps=1)

plt.show()
```

**What This Does:**

- Shows the initial K-Means clustering solution.
- Iteratively updates the cluster assignments and centers for several steps, simulating how K-Means converges.
- After finishing K-Means iterations, switches to the DBSCAN clustering result in the final frame.
- Produces an `.mp4` file that you can open to see the progression of K-Means and the final DBSCAN result.

**Takeaways:**

- The animation helps visualize how K-Means moves cluster centers and gradually stabilizes.
- Comparing the final K-Means outcome (often resulting in roughly circular clusters) with DBSCAN’s result (which can find more complex shapes) highlights how different clustering methods can yield different insights.

----

Explain hierarchical clustering

Hierarchical clustering is a way to group data points into clusters at different levels of similarity. Instead of fixing the number of clusters at the start, it builds a tree-like structure called a dendrogram. This dendrogram shows how data points and clusters merge or split as you move up or down the tree.

There are two main approaches:  
- **Agglomerative (bottom-up):** Start with each data point as its own cluster. Then merge the two closest clusters step by step until you have a single cluster that contains all data points.  
- **Divisive (top-down):** Start with all data points in one large cluster. Then split the cluster into two smaller clusters, and keep splitting until each data point stands alone.

To decide which clusters to merge or split, you need a measure of similarity or distance. Common choices include Euclidean distance between points and the linkages (ways to measure distance between clusters) such as single linkage (closest points), complete linkage (farthest points), or average linkage (average distance between points in different clusters).

Hierarchical clustering helps you understand the structure of your data at multiple levels. You can “cut” the dendrogram at different heights to pick the number of clusters that best suits your task. This flexibility makes hierarchical clustering useful for exploring data before deciding how many groups to form.

## Hierarchical Clustering

An outline and a code example to generate a simple animation (video) that illustrates hierarchical clustering using Python. 

1. Load a dataset (for example, the Iris dataset).
2. Compute a distance matrix and use hierarchical clustering methods.
3. Generate frames that show the step-by-step merging of clusters in an agglomerative manner.
4. Use `matplotlib`'s animation capabilities to create a dendrogram-like animation.
5. Save the animation as a video file (e.g., MP4).

**Steps:**

- Perform hierarchical clustering on the data and obtain the linkage matrix.
- A linkage matrix records how clusters merge at each step.
- Each step reduces the number of clusters by one as two clusters merge.
- We can visualize this process by starting with points as individual clusters and progressively merging them, drawing connecting lines as clusters form.
- Using `matplotlib.animation.FuncAnimation`, we can produce frames showing incremental cluster merges.
- Finally, we can save this animation to a video file.

**Code Example (run locally):**

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from sklearn.datasets import load_iris
from scipy.cluster.hierarchy import linkage

# Load dataset (Iris)
iris = load_iris()
X = iris.data[:, :2]  # just take first two features for easy 2D plotting

# Compute linkage matrix for hierarchical clustering
Z = linkage(X, method='ward')

# Z is an n-1 by 4 matrix. Each row shows:
# [cluster1_index, cluster2_index, distance, new_cluster_size]

# Initial cluster assignments: each point is its own cluster
n_samples = X.shape[0]
clusters = {i: [i] for i in range(n_samples)}

fig, ax = plt.subplots(figsize=(8, 6))

# We will show points and then lines as clusters merge
points = ax.scatter(X[:, 0], X[:, 1], c='blue', s=50, label='Data points')
lines = []

def init():
    ax.set_title("Hierarchical Clustering")
    ax.set_xlabel("Feature 1")
    ax.set_ylabel("Feature 2")
    return [points]

def update(frame):
    # frame corresponds to each merge step
    # Each step merges two clusters
    i1, i2, dist, size = Z[frame]
    i1, i2 = int(i1), int(i2)

    # Find the clusters being merged
    c1 = clusters.pop(i1, None) or [i1 - n_samples]
    c2 = clusters.pop(i2, None) or [i2 - n_samples]

    # Merge them
    merged_cluster = c1 + c2
    # Assign new cluster index to a negative value to avoid confusion with original points
    new_cluster_idx = -(frame + 1)  
    clusters[new_cluster_idx] = merged_cluster

    # Compute mean coordinates of merged cluster for plotting the linkage line
    c1_coords = X[c1, :].mean(axis=0)
    c2_coords = X[c2, :].mean(axis=0)
    merged_coords = (c1_coords + c2_coords) / 2.0

    # Draw a line between the two cluster centroids and a vertical bar up to the merged cluster centroid
    line = ax.plot([c1_coords[0], c2_coords[0]], [c1_coords[1], c2_coords[1]], 'r-')
    # Add a marker at the merged cluster centroid (optional)
    ax.scatter(merged_coords[0], merged_coords[1], c='red', s=50)

    lines.extend(line)
    return line

anim = animation.FuncAnimation(fig, update, frames=range(Z.shape[0]),
                               init_func=init, blit=False, repeat=False)

# Save the animation as a video (requires ffmpeg installed)
anim.save('hierarchical_clustering.mp4', writer='ffmpeg', fps=1)

plt.show()
```

**What This Does:**

- Starts with all individual points as separate clusters.
- Iteratively merges two clusters at a time according to the linkage matrix.
- Draws lines between clusters as they merge, simulating the growth of a dendrogram structure.
- Produces an MP4 file showing the step-by-step clustering process.

**Adjustments and Notes:**

- You can tweak linkage methods (`single`, `complete`, `average`, `ward`) to see different merging patterns.
- You can modify colors, markers, and speeds.
- The final dendrogram animation here is a conceptual visualization, not a traditional dendrogram. But it conveys the idea of clusters merging step by step.
- The generated `.mp4` file can be opened in any standard media player, effectively showing a “video” of hierarchical clustering formation.

----

Explain choosing cluster count with K-means (Elbow Method)

The Elbow Method is a common way to decide how many clusters to use in K-Means. The idea is to run K-Means with different numbers of clusters (for example, from 1 to 10) and measure how well the model fits the data. One common measure is the “within-cluster sum of squares” (WCSS), which is the sum of the squared distances between each point and its assigned cluster’s center.

You then plot the number of clusters on the x-axis and the WCSS value on the y-axis. As you increase the number of clusters, WCSS will decrease since clusters fit the data more closely. However, after a certain point, adding more clusters results in only minor improvements. This point of diminishing returns typically appears as a bend or “elbow” in the plot. The number of clusters at the elbow is often a good choice because it balances clustering quality and complexity.

1. Run K-Means with different cluster counts.
2. Calculate WCSS for each run.
3. Plot the results and look for the “elbow.”
4. Choose the cluster count at the elbow, where adding more clusters does not significantly improve WCSS.

## Implementation

An outline and code example to generate a simple animation (video) that demonstrates the Elbow Method for choosing the number of clusters in K-Means. The animation will show how we compute and plot the within-cluster sum of squares (WCSS) as we vary the number of clusters from 1 to a chosen upper limit.

**Steps:**

1. Load a dataset (e.g., the Iris dataset, focusing on two features for simplicity).
2. For k = 1 to K_max (a chosen maximum number of clusters), run K-Means clustering and compute WCSS.
3. Use `matplotlib.animation.FuncAnimation` to animate the plotting process:
   - At each frame, we show the WCSS value for the current k, gradually building the plot.
4. Save the resulting animation as a video file (e.g., MP4).

**Code Example (run locally):**

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from sklearn.cluster import KMeans
from sklearn.datasets import load_iris

# Load dataset
iris = load_iris()
X = iris.data[:, :2]  # take only the first two features for easy plotting

K_max = 10
wcss_values = []

# Pre-compute WCSS for each k to avoid slowing down the animation
for k in range(1, K_max + 1):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    wcss = kmeans.inertia_  # inertia_ is the sum of squared distances to cluster centers
    wcss_values.append(wcss)

fig, ax = plt.subplots(figsize=(8, 5))
ax.set_xlim(0, K_max + 1)
ax.set_ylim(0, max(wcss_values) * 1.1)
ax.set_xlabel('Number of Clusters (k)')
ax.set_ylabel('WCSS')
ax.set_title('Elbow Method for Choosing K')

line, = ax.plot([], [], marker='o', color='b')
text_annotation = ax.text(0.1, 0.9, '', transform=ax.transAxes)

def init():
    line.set_data([], [])
    text_annotation.set_text('')
    return line, text_annotation

def update(frame):
    # frame goes from 0 to K_max-1
    x_data = list(range(1, frame + 2))   # k values up to current frame
    y_data = wcss_values[:frame + 1]     # corresponding WCSS values
    line.set_data(x_data, y_data)
    text_annotation.set_text(f'k={frame+1}, WCSS={y_data[-1]:.2f}')
    return line, text_annotation

anim = animation.FuncAnimation(fig, update, frames=K_max, 
                               init_func=init, blit=False, repeat=False)

# Save the animation as a video file (requires ffmpeg)
anim.save('elbow_method.mp4', writer='ffmpeg', fps=1)

plt.show()
```

**What This Does:**

- Iterates through possible cluster counts (1 to 10).
- Calculates WCSS for each cluster count.
- Gradually plots points on the “Number of Clusters vs WCSS” curve, simulating the process of deciding how many clusters to choose.
- Shows a text annotation to highlight the current k and WCSS value.
- Produces an MP4 file that animates the building of the elbow plot step-by-step.

**Adjustments and Notes:**

- You can change the dataset or features as needed.
- You can tweak K_max to show more or fewer clusters.
- Once the video is saved, you can open it in a standard media player to visualize how the elbow plot is formed incrementally.
