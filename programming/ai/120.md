Recommendation Systems
Objective - To understand the idea behind Recommendation Systems and potential business applications

Here's a comprehensive breakdown of recommendation systems, their workings, and their value for businesses:

**What are Recommendation Systems?**

Recommendation systems (sometimes called recommender systems) are machine learning algorithms that use data to suggest relevant items or content to users. They aim to provide personalized experiences, filtering through overwhelming amounts of choices to find the most likely options a user would enjoy or find useful.

**How do Recommendation Systems Work?**

Here are the most common techniques used in recommendation systems:

* **Collaborative Filtering:**  The core logic here is "users who liked this also liked that." These systems make recommendations based on the behavior and preferences of similar users. 
    * **User-based:** Examines users with similar tastes to the current user, looking for shared interests to recommend new items.
    * **Item-based:**  Identifies items that are frequently purchased or rated together, then suggests those to users who've liked a similar item.

* **Content-Based Filtering:** This method focuses on "recommending items similar to what you've previously liked."  Recommendations are based on analyzing the attributes of items themselves (like movie genre, director, plot keywords) and matching them to a user's past preferences. 

* **Hybrid Systems:** Many powerful recommendation systems combine  the collaborative and content-based approaches to leverage the strengths of both.

**Business Applications of Recommendation Systems**

1. **E-commerce Product Recommendations:** "You might also like...", "Frequently bought together..." are familiar phrases driven by recommendation engines.  These boost product discovery, cross-selling, and upselling – all ultimately benefiting sales.

2. **Streaming Content Recommendations:** Netflix, YouTube, Spotify, and others personalize your listening, viewing, and reading experiences for increased engagement and time spent on their platforms.

3. **Social Media Feed Curation:** Facebook, Twitter, etc., tailor the content you see based on your interactions and connections, encouraging more scrolling and activity.

4. **Targeted Advertising:** Systems match ads to users with the highest likelihood of interest, resulting in more effective advertising spend.

5. **Personalized Email Marketing:** Recommending items or content based on a subscriber's past interactions improves  email open rates and the chances of them being clicked.

6. **Search Engine Optimization:** Some engines incorporate recommendation-like  signals by matching your query to previously successful searches from other users.  This attempts to give you the most relevant content with fewer search attempts.

**Benefits for Businesses**

* **Increased Sales/Conversions:** Better recommendations lead to greater product/content discovery and increased likelihood of purchase or engagement.
* **Enhanced Customer Experience:** Personalization makes users feel understood, improves their perception of a company, and fosters loyalty.
* **Data-Driven Insights:** Analysis of recommendation system data  reveals patterns in customer preference, behavior, and market trends.
* **Competitive Advantage:** Offering high-quality, personalized recommendations differentiates businesses in a crowded market.

Building a high-performing recommendation system involves data science expertise, a focus on continuous improvement, and ethical considerations around data privacy and usage.

**Want an Example?**

Let's assume you're on Amazon. It likely has stored:

* Your past purchases/wish list items
* Products you've browsed (even without purchase)
* Items you've rated and the type of feedback given

Combining that with purchase patterns of millions of other users, the recommendation system works its magic to suggest additional items likely to fit your interests.

----

Write a python program to demonstrate Collaborative Filtering: User-based and Item-based

Below is an example Python program that demonstrates the basic concepts of user-based and item-based collaborative filtering using a small, synthetic rating matrix. The code is self-contained and shows:

1. A small user-item rating matrix (e.g., rows as users, columns as items).
2. Calculation of user-user similarity and predictions for missing ratings based on user-based collaborative filtering.
3. Calculation of item-item similarity and predictions for missing ratings based on item-based collaborative filtering.

In a real application, you would have a larger, sparser dataset and might use more sophisticated similarity measures (e.g., cosine similarity, Pearson correlation) and potentially normalization techniques. This example focuses on the core logic in a simplified manner.

**Key Steps in the Example:**
- Create a small rating matrix with some missing values (0 represents missing ratings).
- Compute similarity between users and between items.
- Predict missing ratings:
  - User-based: Use similar users’ ratings to predict an item for a target user.
  - Item-based: Use similar items’ ratings to predict what a user would rate an item they haven’t rated.

```python
import numpy as np

# Example rating matrix
# Rows: Users (U1, U2, U3, U4)
# Columns: Items (I1, I2, I3, I4, I5)
# 0 means no rating provided
ratings = np.array([
    [4, 0, 3, 5, 0],  # User 1
    [5, 4, 0, 4, 2],  # User 2
    [0, 2, 4, 0, 1],  # User 3
    [3, 0, 2, 4, 4]   # User 4
])

# A simple similarity function (cosine similarity)
def cosine_similarity(a, b):
    a = np.array(a)
    b = np.array(b)
    num = np.dot(a, b)
    den = (np.linalg.norm(a) * np.linalg.norm(b))
    return num/den if den != 0 else 0

# USER-BASED COLLABORATIVE FILTERING
# 1. Compute user-user similarity matrix
num_users, num_items = ratings.shape
user_similarity = np.zeros((num_users, num_users))

for u in range(num_users):
    for v in range(num_users):
        if u != v:
            # Consider only items rated by both users to compute similarity
            # Here we keep it simple and use all items, ignoring missing (0) could be more complex
            user_similarity[u, v] = cosine_similarity(ratings[u], ratings[v])

# Function to predict rating for user u, item i using user-based filtering
def predict_user_based(u, i, k=2):
    # Find users who rated item i
    users_who_rated = np.where(ratings[:, i] > 0)[0]
    # If no one rated the item, return a default value (e.g., average user rating)
    if len(users_who_rated) == 0:
        user_avg = np.mean(ratings[u, ratings[u] > 0]) if np.any(ratings[u] > 0) else 3.0
        return user_avg
    
    # Get similarities of user u to other users
    sims = user_similarity[u, users_who_rated]
    
    # Pick top k similar users who rated item i
    top_k_idx = np.argsort(sims)[-k:]
    top_k_users = users_who_rated[top_k_idx]
    top_k_sims = sims[top_k_idx]
    
    # Compute weighted average of their ratings
    numerator = np.sum(top_k_sims * ratings[top_k_users, i])
    denominator = np.sum(np.abs(top_k_sims)) if np.sum(np.abs(top_k_sims)) != 0 else 1
    return numerator/denominator

# ITEM-BASED COLLABORATIVE FILTERING
# 1. Compute item-item similarity matrix
item_similarity = np.zeros((num_items, num_items))

# Transpose ratings for item-based similarity: consider each column as a vector
item_vectors = ratings.T
for i in range(num_items):
    for j in range(num_items):
        if i != j:
            item_similarity[i, j] = cosine_similarity(item_vectors[i], item_vectors[j])

# Function to predict rating for user u, item i using item-based filtering
def predict_item_based(u, i, k=2):
    # Find items rated by user u
    items_rated = np.where(ratings[u] > 0)[0]
    # If user hasn't rated anything, return a default value (e.g., global average)
    if len(items_rated) == 0:
        return np.mean(ratings[ratings > 0]) if np.any(ratings > 0) else 3.0
    
    # Get similarities of item i to other items user u rated
    sims = item_similarity[i, items_rated]
    
    # Pick top k similar items
    top_k_idx = np.argsort(sims)[-k:]
    top_k_items = items_rated[top_k_idx]
    top_k_sims = sims[top_k_idx]
    
    # Compute weighted average of their ratings
    numerator = np.sum(top_k_sims * ratings[u, top_k_items])
    denominator = np.sum(np.abs(top_k_sims)) if np.sum(np.abs(top_k_sims)) != 0 else 1
    return numerator / denominator

# Demonstration:
# Let's pick a user-item pair where the rating is missing and try to predict it
target_user = 0  # User 1
target_item = 1  # Item 2 (which User 1 hasn't rated)

print("Original Ratings Matrix:")
print(ratings)
print("\nUser-User Similarity Matrix:")
print(user_similarity)
print("\nItem-Item Similarity Matrix:")
print(item_similarity)

pred_user_based = predict_user_based(target_user, target_item, k=2)
pred_item_based = predict_item_based(target_user, target_item, k=2)

print(f"\nPredicting rating for User {target_user+1}, Item {target_item+1}:")
print(f"User-based prediction: {pred_user_based:.2f}")
print(f"Item-based prediction: {pred_item_based:.2f}")
```

**What this code does:**

- It sets up a small rating matrix with some zero entries indicating that the user hasn't rated those items.
- It then computes user-user similarities and item-item similarities.
- Using these similarities, it demonstrates how to predict a missing rating for a given user and item using both user-based and item-based collaborative filtering approaches.
- In a real-world scenario, you would integrate more robust similarity measures, handle sparse data at scale, and incorporate normalization or mean-centering.

----

Write a python program to demonstrate Content-Based Filtering

An example Python program that demonstrates the basic idea of content-based filtering.

1. Create a small set of "items" (e.g., movies) and their associated features (e.g., genre, year, director).
2. Represent each item as a vector of features.
3. Take a user’s liked item (or user’s profile) and calculate the similarity of other items to that liked item.
4. Recommend items with the highest similarity scores.

**Key Points in this Example:**
- We'll use a small dataset of movies and their features.
- We will convert categorical features into a simplified vector (e.g., one-hot encoding genres).
- We'll compute similarity using cosine similarity.
- Then we will pick a movie the user likes and recommend similar movies based on content features.

**Code Example:**
```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Suppose we have a small movie dataset with features: Genre, Year, and Director.
# Genre: We'll represent as a multi-hot vector for simplicity.
# For example: Genre vector order: [Action, Comedy, Drama, Romance]
# Director: We'll encode as a one-hot vector as well. For simplicity, assume we have 3 directors.
# Director vector order: [Dir_A, Dir_B, Dir_C]

# Let's define a small set of movies
movies = [
    {"title": "Movie A", "genre": ["Action", "Comedy"], "year": 2010, "director": "Dir_A"},
    {"title": "Movie B", "genre": ["Drama"], "year": 2012, "director": "Dir_B"},
    {"title": "Movie C", "genre": ["Comedy", "Romance"], "year": 2008, "director": "Dir_C"},
    {"title": "Movie D", "genre": ["Action"], "year": 2011, "director": "Dir_B"},
    {"title": "Movie E", "genre": ["Drama", "Romance"], "year": 2009, "director": "Dir_A"}
]

# Define mappings to numerical features
genre_list = ["Action", "Comedy", "Drama", "Romance"]
director_list = ["Dir_A", "Dir_B", "Dir_C"]

def encode_movie(movie):
    # Encode genre
    genre_vector = [1 if g in movie["genre"] else 0 for g in genre_list]
    # Encode director
    director_vector = [1 if movie["director"] == d else 0 for d in director_list]
    # Encode year as a scaled value (for simplicity, just normalize by dividing by 2020)
    year_value = movie["year"] / 2020.0
    # Final feature vector: [genres..., directors..., year]
    return np.array(genre_vector + director_vector + [year_value])

# Build a feature matrix for all movies
feature_matrix = np.array([encode_movie(m) for m in movies])

# Let's say the user likes "Movie A". We want to recommend similar movies based on content.
user_liked_movie_title = "Movie A"
user_idx = [i for i,m in enumerate(movies) if m["title"] == user_liked_movie_title][0]

user_movie_vector = feature_matrix[user_idx].reshape(1, -1)

# Compute similarity of all movies to the user's liked movie
similarities = cosine_similarity(user_movie_vector, feature_matrix)[0]

# Sort movies by similarity score (excluding the movie itself)
similar_items = [(movies[i]["title"], similarities[i]) for i in range(len(movies)) if i != user_idx]
similar_items.sort(key=lambda x: x[1], reverse=True)

print(f"User likes: {user_liked_movie_title}")
print("Recommended based on content similarity:")
for title, score in similar_items:
    print(f"{title} (similarity: {score:.2f})")
```

**What this code does:**

- It defines a small movie dataset and encodes each movie into a numeric feature vector that captures genre, director, and year.
- It then uses cosine similarity to measure how similar each movie is to a movie the user already likes.
- Finally, it prints out a list of recommendations ordered by their similarity scores.

**Extending the Approach:**

In a real-world scenario:

- Use more diverse and descriptive features (plot keywords, cast, language, etc.).
- Preprocess and normalize features.
- Possibly use embeddings or more sophisticated representations for text-based features.
- Integrate user profiles if available (e.g., aggregate liked item features to form a user preference vector).

This example focuses on demonstrating the core logic of content-based filtering in a simple, easy-to-understand way.
