Decision Systems

Objective: To understand the concept of classification and understand how Tree-Based models achieve prediction of outcomes that fall into 2 or more categories.

- Understand the Decision Tree Model and the mechanics behind its predictions
- Learn to evaluate the performance of classification models
- Understand the concepts of Ensemble Learning and Bagging
- Learn how Random Forests aggregate the predictions of multiple Decision Trees

Let's delve into the realm of classification and focus on powerful tree-based models for this task.  

**Classification: Sorting Things into Categories**

* **The Core Idea:** Unlike regression (predicting numerical values), classification deals with placing data into discrete classes. Examples:
    * Email filtering: Spam or not spam?
    * Medical diagnosis: Does this scan indicate a specific disease or not?
    * Customer churn prediction: Is this customer likely to leave or stay?

* **Labels Are Key:** Supervised learning at its heart. Models are trained on a dataset where each example has its true label ("target" variable) already present.  

**Why Tree-Based Models Excel**

* **Logic:** Humans often reason in a branching, 'if-then-else' manner. Decision trees mirror this with a series of splits.
* **How a Tree 'Learns':**
   * Starts at the root with the whole dataset.
   * Iteratively chooses a feature and splitting value. This divides the data into subsets, aiming for each split to create groups with increasingly 'purer' class labels (fewer mixed examples of spam/no spam, etc., in a split group).
   * Tree depth and splitting criteria control complexity and prevent overfitting.
* **Handling Different Data:** Works for both numerical features (splitting like "income < $50K") and categorical ones ("state of residence is California").

**Advantages of Decision Trees**

* **Interpretability:** You can literally trace a decision pathway along the tree branches, seeing why a classification was made. This is unlike "black-box" methods like some neural networks. 
* **Robust to Outliers:**  By isolating a few atypical examples in a single leaf node, they influence the overall model less than in techniques like linear regression. 
* **Minimal Assumptions:** They don't require features to be on the same scale (standardization is less crucial) or to have linear relationships. 

**Beyond Basic Decision Trees**

* **Random Forests:** Build an ensemble (collection) of many decision trees. Each is trained on slightly different data or feature subsets. Averaging across their predictions makes results more robust and less prone to overfitting than a single tree.
* **Gradient Boosting (XGBoost, LightGBM):** Sequentially build trees; each  focuses on correcting the mistakes of the ones before it. Powerful, often yields top performance in data science competitions. 

**Caveats**

* **Potential for Overfitting:** Especially with single deep trees. Hyperparameter tuning (limits on tree depth, minimum samples per leaf node, etc.)  mitigates this.
* **Sensitivity to Data Changes:** A small shift in training data may generate a wildly different tree structure. Ensemble methods help this instability.

**Application Examples**

Let's pick a domain or task, and I can outline which features make good splitting candidates and discuss why tree-based models shine in that scenario. Here are some options:

* **Marketing: Propensity to Buy** Analyzing demographics, browsing behavior, and  past purchases to predict who'll respond to a campaign.
* **Finance: Fraud Detection**  Pattern spotting in  transaction data to flag suspicious activity deviating from 'normal' for an account type.
* **Anything With Many Interacting Features** Where simple linear relationships fall short; trees find  combinations that strongly signal a class.

* **Visualize a simple decision tree structure?**  illustrate this with a hypothetical dataset
* **Discuss code walkthrough:** See how  models like these are built using Scikit-learn (Python) or another library
* **Talk about tuning strategies:** Tradeoffs between tree complexity, bias, and variance for optimizing classification performance?

Let's visualize a simple decision tree to illustrate how they work. We'll use a hypothetical dataset where we're trying to predict whether someone is likely to enjoy a new action movie.

**Features (Inputs) for our Decision Tree**

* **Age:** A numerical feature 
* **Likes Sci-Fi:**  A categorical feature (Yes/No)
* **Enjoys Thrillers:**  Categorical feature (Yes/No)

**Outcome (Target)**

* **Will Enjoy Action Movie:** (Yes/No)

**Simple Visualization**

Here's a rudimentary, but conceptually representative, tree structure:

```
         Root Node (All Data)
                /        \
      Age < 30?          Age >= 30 
       /    \             /        \
   Yes      No         Yes        No
  /  \       \        /   \
Likes Sci-Fi?  Enjoy Thrillers?
  / \          / \  
Yes No      Yes  No
 \  /         \  /
Predict: YES  Predict: NO  
```

**How to Read This Tree**

1.  **Root Node:**  Where it all starts! Contains our entire dataset before any classification splits. 
2.  **Decision Rule:** Each node has a question splitting the data. "Age < 30?" is our first such rule.
3.  **Branches:** Data gets classified down Yes/No paths based on the condition
4.  **Leaf Nodes:** The end of the line!  Based on the pattern learned from the training data, this is where a prediction is made. In our case,  "Predict: YES" implies the model believes there's a higher chance that someone fitting that path of rules will enjoy the action movie. 

**Notes**

* **Feature Importance:** The feature chosen at the very top  would be considered most discriminative by the tree building algorithm (here, it's age).
* **Purity:** Ideally, leaf nodes will contain mostly one class (all who fit here LOVE action movies, vs. the ones here who DON'T), meaning confident predictions. Mixed-class leaf nodes imply uncertainty.
* **This is Tiny:** Real trees are large! This gives the flavor; they become hard to hand-draw  past a few branching levels.

**Making It Concrete (With Fictional Data)**
 
Imagine our training data had 100 people of all ages watching the movie, noting those features along with 'liked'/'disliked' responses. The tree wouldn't be arbitrary. Our diagram reflects what patterns this data MAY exhibit:

* "Age < 30" as the top split suggests younger folks were more likely to enjoy the movie OVERALL, regardless of other features.
* Among teens into Sci-Fi â€“ likely action movie viewers anyway. However, even Sci-Fi disliking youngsters had a decent chance of enjoying THIS movie in our imaginary training data. 

* Change the splitting rules or modify the 'predictions' on our little diagram to test
* Brainstorm a small dataset and outline a more detailed tree that might emerge from that scenario

Let's see Scikit-learn in action for building tree-based classification models in Python.

**Setup**

Make sure you have these installed:

* Python ([https://www.python.org/](https://www.python.org/))
* Scikit-learn (`pip install scikit-learn`) 
* pandas (data manipulation): `pip install pandas`

**A Classic Dataset: Iris**

We'll use the Iris dataset, classifying iris flower types based on their petal and sepal dimensions. It's built into Scikit-learn so we don't need to download.

**Basic Code Structure**

```python
import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics

# Load the Iris dataset 
iris = datasets.load_iris()

# Data into a DataFrame for  convenience
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['target'] = iris.target 

# Features  as  'X', the thing we want to predict as 'y'
X = df.drop('target', axis=1)
y = df['target']

# Split into training and testing sets (common practice)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Create a decision tree classifier object (using defaults for now)
model = DecisionTreeClassifier()

# Train the model  
model.fit(X_train, y_train)

# Use the model for prediction on test data 
y_pred = model.predict(X_test)

# Basic evaluation 
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))
```

**Explanation**

1. **Imports:** Loading  libraries - pandas for DataFrame, datasets from Scikit-learn, model import, and an accuracy metric.
2. **Loading Iris:** `datasets.load_iris()`, the built-in method
3. **Data Wrangling:** Creating a DataFrame (table-like) for easy handling. Features go into 'X', and the flower type we're predicting (0, 1, 2 for the species) into 'y'.
4. **Train/Test Split:**  Holding back some data for  testing prevents our model from just 'memorizing' seen examples. 
5. **Model Instantiation:** `DecisionTreeClassifier()` creates our classifier.
6. **Training (the Magic):**  `model.fit()`, where  the algorithm finds tree splits optimal to predict 'y' from 'X'.
7. **Prediction Time:**  Our unseen test data (`X_test`) is fed to the  trained model, generating predictions in `y_pred`.
8. **Evaluation:** Checking how often predictions matched the true labels ('y_test').

Scikit-learn offers control:

* `max_depth`: Limits how 'tall' your tree grows
* `criterion`: 'gini' or 'entropy' influence split decision calculations (purity).
* Visualizations: Libraries like Graphviz let you render the tree!

We can:

* Explore how changing those parameters affects performance on our flower classification
* Try Random Forests: `RandomForestClassifier` and discuss the ensemble principle
* Have a specific classification problem with data we could load
