Prediction Methods - Regression

Objective - To understand the concept of Linear Regression and how it can be used with historical data to build models that can predict future outcomes.

- The idea of regression and predicting a continuous output
- How do you build a model that best fits your data?
- How do you quantify the degree of uncertainty?
- What do you do when you don't have enough data?
- What lies beyond Linear Regression?

Regression analysis, unpacking linear regression to predict continuous outcomes using historical data.

**Regression: Predicting the 'How Much'**

* Regression is about finding relationships between a dependent variable (thing you want to predict) and one or more independent variables (features that influence the prediction).
* **Continuous vs. Classification:** Think of regression as forecasting a number (e.g., expected sales, stock price tomorrow, website load time) in contrast to classification, which places data points into categories (spam vs. not spam, will buy vs. won't buy).

**Linear Regression:  Keeping it Simple (Yet Powerful)**

* **The Line of Best Fit:** Imagine a scatter plot of your data, with the outcome you want to predict on the y-axis. Linear regression aims to draw the 'best fitting' straight line through it.
* **The Equation:** A line is mathematically expressed as:  Y = b0 + b1*X   Where:
     * Y is the predicted value 
     * X is your input feature
     * b0 is the y-intercept (where the line crosses the Y-axis)
     * b1 is the slope (steepness of the line, signifying how strongly X influences Y)
* **Finding the 'Best':** This means minimizing errors – the difference between your line's prediction and the observed data points. Commonly used method: Least Squares (minimizing sum of squared errors).

**Quantifying Uncertainty**

* **Not Perfect Predictions:** Regression rarely makes 100% accurate predictions. There's noise and randomness in the real world.
* **R-squared:** A key metric indicating how well your model explains the variation in the data. An R-squared of 1 implies a perfect fit (rare!), while lower values imply data the model isn't fully capturing. 
* **Confidence Intervals:** Provides a range around your prediction, signifying the degree of uncertainty. For example, "We predict sales to be $10,000 with a 95% confidence interval between $8,000 and $12,000"

**When 'Not Enough' Is a Problem** 

* **Overfitting:** A complex model trained on too little data can memorize the existing data but perform poorly on unseen examples. Look for a large gap between performance on your training vs. a held-out testing set. 
* **Techniques to Combat:**
    * Regularization (L1/L2): Penalizes overly complex models
    * Cross-Validation: Smartly splitting data to get robust performance estimates
    * Data Augmentation: If applicable, generate synthetic data to expand your training set.

**Beyond Linear Regression**

* **Nonlinear Relationships:** For curves and complex patterns, non-linear methods like decision trees, polynomial regression, or neural networks come to the rescue.  
* **Multiple Features:** Multiple Linear Regression handles several independent variables influencing your outcome. 
* **Assumptions are Important:** Linear regression thrives under some assumptions about your data (linearity, errors being normally distributed, etc.). It's vital to check if these hold.

* **A worked example (calculation-focused):** Predicting house prices based on their square footage?
* **Code implementation:**  Performing linear regression using Python (scikit-learn) or R and interpreting the results?
* **A specific industry use-case:** Analyzing regression applications in finance, healthcare, or marketing?

**Option 1: Worked Example (Calculation-Focused)**

* **The Data:** We could use a simple dataset on house prices vs. square footage. I can either provide this, or we can search for public housing price datasets suitable for a tutorial exercise.
* **Step by Step:** We would guide you through:
    * Visualizing the data initially to spot any linear pattern.
    * The core linear regression calculation (you could follow along in Excel for simplicity, or jump straight to a programming environment if you're comfortable). 
    * Understanding the interpretation of the slope coefficient (how much an extra square foot adds to the predicted price).
    * Calculating measures like R-squared to see how good the fit is.

**Option 2: Code Implementation**

* **Language Choice:** Let's go with Python (using the Scikit-learn library) or R. 
* **Dataset:** We can either keep the house price example or find a slightly more interesting dataset that has a clear "target" variable to predict.
* **Focus:** In this option, we would emphasize:
    * Loading the data and exploring it briefly.
    * Preprocessing (if needed) and splitting into training and test sets.
    * Creating the regression model, fitting, and making predictions.
    * Evaluating performance metrics (R-squared, residual analysis, etc.). 

**Option 3: Industry Use-Case**

 Do you have a particular industry in mind: finance, healthcare, marketing...?  We can pick examples relevant to that field to illustrate the power of regression.
* **Style:** Here, we could offer some less code-focused, and more concept-driven cases:
    * Example: Risk scoring in loan applications (what features, model type)
    * Forecasting product demand for inventory management based on seasonality and marketing promotions.
    * Discussion of challenges specific to the industry (e.g., ethical considerations when dealing with healthcare data).

## Real Estate Example

Let's dive into a worked example – seeing the calculations will enhance your understanding of linear regression's inner workings.

**The Scenario: House Prices and Square Footage**

Imagine you're a real estate analyst. You want to see if there's a simple way to predict house prices based on their size. There might be a relationship, but let's quantify it.

**Our Dataset (Simplified)**

| Square Footage | House Price ($1000s) |
|----------------|---------------------|
| 1200           | 230                 |
| 1550           | 310                 |
| 800            | 185                 |
| 2100           | 400                 |
| ...            | ...                 |  (a few more rows)

**Step 1: Visualization**

* Always plot first! Create a scatter plot with Square Footage on the X-axis and House Price on the Y-axis.
* Do you observe a roughly linear trend (points scattered somewhat around an upward-sloping imagined line)? 

**Step 2: Finding the Line of Best Fit**

Let's simplify and NOT use libraries at first. Here's the core method (you could follow in Excel):

1. **Means:** Calculate the average square footage (X_mean) and the average house price (Y_mean).
2. **Numerator:** For each house: 
    * (Subtract X_mean from its square footage) and multiply by (Subtract Y_mean from its price). 
    * Sum up these calculations across all houses.
3. **Denominator:** For each house: 
    * (Subtract X_mean from its square footage) and square the result.
    * Sum these up across all houses.
4. **Slope (b1):**  Numerator from step 2 / Denominator from step 3
5. **Intercept (b0):** Y_mean - (b1 * X_mean)

**Step 3: The Equation**

* With our calculated slope (b1) and intercept (b0), we have the equation of our best-fit line:  Predicted House Price = b0 + b1 * Square Footage

**Step 4: Interpretation**

* **Slope:** Tells us for every extra square foot,  our model predicts the price changes by  'b1'  thousands of dollars. Positive 'b1' implies prices increase with larger houses, as expected.
* **R-squared:**  We'd need to calculate this manually based on prediction errors of our model versus the mean price. It gives us a sense of how well the line explains price variations. 

**Notes**

* **Real-world datasets are messier!** Outliers heavily skew this. This example gives you the 'under the hood' essence.
* **Libraries make this a breeze:** Scikit-learn has a single function; however, knowing the process gives you  intuition.

* Walk through this calculation step-by-step with our example data and arrive at the numerical equation
* Discuss why a large R-squared doesn't always mean an excellent model in practical settings.
* Introduce a SECOND feature to our prediction, say number of bedrooms (multiple linear regression)

## Implementation

Objective - To understand the concept of Linear Regression and how it can be used with historical data to build models that can predict future outcomes.

Imagine you're a real estate analyst. You want to see if there's a simple way to predict house prices based on their size. There might be a relationship, but let's quantify it. Walk through this calculation step-by-step with our example data and arrive at the numerical equation.

An example illustrating how to perform a basic linear regression calculation by hand. Assume we have the following data, representing house sizes (in square feet) and their corresponding sale prices (in thousands of dollars):

| House # | Size (x) | Price (y) |
|---------|-----------|-----------|
| 1       | 1000      | 200       |
| 2       | 1200      | 230       |
| 3       | 1500      | 270       |
| 4       | 2000      | 320       |
| 5       | 2200      | 350       |

**Step 1: Compute the means of x and y.**

- Mean of x (house size):
  
  \[
  \bar{x} = \frac{1000 + 1200 + 1500 + 2000 + 2200}{5} = \frac{8900}{5} = 1780.
  \]

- Mean of y (house price):
  
  \[
  \bar{y} = \frac{200 + 230 + 270 + 320 + 350}{5} = \frac{1370}{5} = 274.
  \]

**Step 2: Calculate the numerator and denominator for the slope formula.**

We use the formula for the slope \( m \):
\[
m = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}.
\]

First, compute \((x_i - \bar{x})\) and \((y_i - \bar{y})\) for each data point, and then their products:

| House # | x   | y   | x - x̄  | y - ȳ  | (x - x̄)(y - ȳ) | (x - x̄)²     |
|---------|-----|-----|---------|---------|------------------|---------------|
| 1       |1000 |200  |1000-1780=-780  |200-274=-74    |(-780)(-74)=57720    |(-780)²=608400 |
| 2       |1200 |230  |1200-1780=-580  |230-274=-44    |(-580)(-44)=25520    |(-580)²=336400 |
| 3       |1500 |270  |1500-1780=-280  |270-274=-4     |(-280)(-4)=1120      |(-280)²=78400  |
| 4       |2000 |320  |2000-1780=220   |320-274=46     |(220)(46)=10120      |(220)²=48400   |
| 5       |2200 |350  |2200-1780=420   |350-274=76     |(420)(76)=31920      |(420)²=176400  |

Now sum the columns:

- \(\sum (x - \bar{x})(y - \bar{y}) = 57720 + 25520 + 1120 + 10120 + 31920 = 126400.\)
- \(\sum (x - \bar{x})^2 = 608400 + 336400 + 78400 + 48400 + 176400 = 1,248,000.\)

**Step 3: Compute the slope \(m\).**

\[
m = \frac{126400}{1,248,000} \approx 0.1013.
\]

This suggests that for each additional square foot, the price goes up by about 0.1013 thousand dollars (or about \$101.30).

**Step 4: Compute the intercept \(b\).**

The intercept \( b \) is given by:
\[
b = \bar{y} - m \bar{x}.
\]

Substitute the values:
\[
b = 274 - (0.1013 \times 1780) \approx 274 - 180.314 \approx 93.686.
\]

This is in thousands of dollars. It represents the predicted price when the house size is zero, which is not meaningful in a real context, but mathematically it sets the baseline.

**Step 5: Write the final linear model equation.**

The linear regression equation is:
\[
\hat{y} = b + m x.
\]

Substitute \( b \) and \( m \):
\[
\hat{y} = 93.686 + 0.1013x.
\]

Interpreting this:
- If you have a house of size \( x \) square feet, the predicted price (in thousands of dollars) is \( 93.686 + 0.1013 \times x \).
- For a 2000 sq ft house, for example:
  \[
  \hat{y} = 93.686 + 0.1013 \times 2000 \approx 93.686 + 202.6 \approx 296.286 \text{ (thousands of dollars)}.
  \]

By following these steps—calculating means, differences from the means, and using them to find the slope and intercept—we derived a simple linear model. This model uses historical data to estimate how house prices change with house size.

----

Discuss why a large R-squared doesn't always mean an excellent model in practical settings.

A large R-squared value shows that a model’s predictions explain a high percentage of the variation in the target variable. This can be appealing, but it does not guarantee that the model is truly useful or that it will perform well on new, unseen data. There are several reasons:

**Overfitting**: 

A model might fit the training data very closely and achieve a high R-squared, yet fail to generalize. When the model is too tailored to the specific noise and patterns of the given dataset, its predictive performance on new data suffers.

**Lack of Causality**: 

A high R-squared only tells us about correlation, not causation. Even if the model fits the data well, it doesn’t mean that the predictors cause the outcome. If the goal is to inform decisions or understand underlying mechanisms, high R-squared alone may be misleading.

**Omitted Variable Bias**: 

A model might have high R-squared but leave out important factors. These missing variables may reduce the practical utility of the model. The model could appear strong within a limited scope but fail to capture other drivers of the outcome.

**Domain Context Matters**: 

Even with a high R-squared, the model might not be practically useful if the underlying variables are too hard to measure, too costly to collect, or too unstable over time. A great statistical fit does not automatically translate into actionable, stable predictions.

R-squared is just one metric. Practical evaluation also requires checking out-of-sample performance, understanding causal relationships, and considering the model’s complexity, stability, and overall usefulness within the real-world domain.

----

Introduce a SECOND feature to our prediction, say number of bedrooms (multiple linear regression)

Let’s consider a scenario where we now have two features: house size (square feet) and number of bedrooms. We believe both may influence price. The idea is similar to simple linear regression, but now we have multiple predictors. Our model equation becomes:

\[
\hat{y} = b_0 + b_1 x_1 + b_2 x_2
\]

Where:  
- \(\hat{y}\) is the predicted price (in thousands of dollars).  
- \(x_1\) is the house size (in square feet).  
- \(x_2\) is the number of bedrooms.  
- \(b_0\) is the intercept.  
- \(b_1\) and \(b_2\) are the coefficients that measure how changes in each feature affect the predicted price.

**Step-by-Step Example:**

Suppose we have the following data:

| House # | Size (x₁) | Bedrooms (x₂) | Price (y) |
|---------|-----------|---------------|-----------|
| 1       | 1000      |  2            | 200       |
| 2       | 1200      |  2            | 230       |
| 3       | 1500      |  3            | 270       |
| 4       | 2000      |  3            | 320       |
| 5       | 2200      |  4            | 350       |

(Prices are still in thousands of dollars.)

**1. Compute Means:**

\[
\bar{x}_1 = \frac{1000 + 1200 + 1500 + 2000 + 2200}{5} = 1780
\]
\[
\bar{x}_2 = \frac{2 + 2 + 3 + 3 + 4}{5} = \frac{14}{5} = 2.8
\]
\[
\bar{y} = \frac{200 + 230 + 270 + 320 + 350}{5} = 274
\]

**2. Set up the Equations:**

The coefficients \(b_0, b_1, b_2\) in multiple linear regression are found by solving a system of linear equations or using matrix methods (like the Normal Equation). The basic formula involves partial derivatives that minimize the sum of squared residuals. While doing this by hand is more involved than the single-feature case, the principle remains the same:

- Compute cross-terms \((x_1 - \bar{x}_1)(y - \bar{y})\), \((x_2 - \bar{x}_2)(y - \bar{y})\), and also consider terms like \((x_1 - \bar{x}_1)(x_2 - \bar{x}_2)\).
- Solve a set of linear equations that come from:
  \[
  b_1 = \frac{\sum (x_1 - \bar{x}_1)(y - \bar{y}) - b_2 \sum (x_1 - \bar{x}_1)(x_2 - \bar{x}_2)}{\sum (x_1 - \bar{x}_1)^2}
  \]
  
  and similarly for \(b_2\).

**3. Matrix Form (Conceptual):**

In practice, we typically rely on matrix operations or software tools:
\[
\mathbf{X} = 
\begin{bmatrix}
1 & x_{11} & x_{12}\\
1 & x_{21} & x_{22}\\
1 & x_{31} & x_{32}\\
1 & x_{41} & x_{42}\\
1 & x_{51} & x_{52}
\end{bmatrix},
\quad
\mathbf{y} = 
\begin{bmatrix}
y_1 \\
y_2 \\
y_3 \\
y_4 \\
y_5
\end{bmatrix}
\]

The solution is given by the Normal Equation:
\[
\mathbf{b} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y},
\]

where \(\mathbf{b} = [b_0, b_1, b_2]^T\).

**4. Interpretation:**

After computing these values (usually via a statistical package or linear algebra), we might arrive at an equation like:

\[
\hat{y} = b_0 + b_1 (\text{size}) + b_2 (\text{bedrooms})
\]

Example (not computed here fully, just illustrating form):

\[
\hat{y} = 50 + 0.09 \times (\text{size}) + 20 \times (\text{bedrooms})
\]

This would mean:
- When size increases by one square foot, price increases by \$90 (0.09 thousand).
- Each additional bedroom adds \$20,000 to the predicted price, all else being equal.

**5. Why Add a Second Feature?**

By introducing a second feature, we can capture more of what drives price differences. A single variable (size) might not fully explain price. Adding the number of bedrooms provides another dimension, potentially improving the model’s predictive accuracy and better reflecting real-world factors that influence prices.

In summary, moving from simple linear regression to multiple linear regression involves using more than one feature, solving a slightly more complex system (usually via software), and interpreting multiple coefficients. This approach can yield a more complete and realistic predictive model.
