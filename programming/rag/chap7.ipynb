{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00884604-ca2b-4bb7-8463-664f31e0bc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==4.3.3 in /Users/bparanj/Library/Python/3.12/lib/python/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim==4.3.3) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/bparanj/Library/Python/3.12/lib/python/site-packages (from gensim==4.3.3) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/bparanj/Library/Python/3.12/lib/python/site-packages (from gensim==4.3.3) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim==4.3.3) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping nougat-ocr as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers==4.45.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers==4.45.1) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers==4.45.1) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers==4.45.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers==4.45.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers==4.45.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers==4.45.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers==4.45.1) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers==4.45.1) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers==4.45.1) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers==4.45.1) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers==4.45.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers==4.45.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers==4.45.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers==4.45.1) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch==2.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.4.1) (69.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch==2.4.1) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch==2.4.1) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/bparanj/Library/Python/3.12/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.0-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.0 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim==4.3.3 --user\n",
    "%pip uninstall -y nougat-ocr\n",
    "%pip install transformers==4.45.1\n",
    "%pip install torch==2.4.1\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2075bf4b-d995-43fd-bc97-6277319be195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USER_AGENT'] = 'RAGUserAgent'\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "import openai\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cf27a84f-dac1-481e-a5b8-725c7e198aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(model_name = \"gpt-4o-mini\")\n",
    "user_query = \"What are the advantages of using RAG?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c4bc164-8535-4a81-9b33-655774f122f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User question embedding (first 5 dimensions): [-0.006360762752592564, -0.0023306477814912796, 0.015498020686209202, -0.022673549130558968, 0.01783391460776329]\n"
     ]
    }
   ],
   "source": [
    "question_embedding = embedding_function.embed_query(user_query)\n",
    "first_5_numbers = question_embedding[:5]\n",
    "print(f\"User question embedding (first 5 dimensions): {first_5_numbers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b7cbd8c-5e3e-4319-8c93-e14de32bb4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 1536\n"
     ]
    }
   ],
   "source": [
    "embedding_size = len(question_embedding)\n",
    "print(f\"Embedding size: {embedding_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1662d0aa-0866-4195-8322-437fe30945e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=[\"https://kbourne.github.io/chapter1.html\"],\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "                                   )\n",
    "    )\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "398036a4-ca86-4e57-8304-4c3ee07eff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(embedding_function)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "76562ec8-373d-4b6d-892b-4d9691eae027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidf_documents = [split.page_content for split in splits]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(tfidf_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6f2656ac-941c-45c2-b4e6-30e42607458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tTF\t\tIDF\n",
      "----\t\t--\t\t---\n",
      "000         \t0.16\t\t2.95\n",
      "1024        \t0.04\t\t2.95\n",
      "123         \t0.02\t\t2.95\n",
      "13          \t0.04\t\t2.95\n",
      "15          \t0.01\t\t2.95\n",
      "16          \t0.07\t\t2.95\n",
      "192         \t0.06\t\t2.95\n",
      "1m          \t0.08\t\t2.95\n",
      "200         \t0.08\t\t2.95\n",
      "2024        \t0.01\t\t2.95\n"
     ]
    }
   ],
   "source": [
    "vocab = tfidf_vectorizer.get_feature_names_out()\n",
    "tf_values = tfidf_matrix.toarray()\n",
    "idf_values = tfidf_vectorizer.idf_\n",
    "word_stats = list(zip(vocab, tf_values.sum(axis=0), idf_values))\n",
    "word_stats.sort(key = lambda x: x[2], reverse=True)\n",
    "print(\"Word\\t\\tTF\\t\\tIDF\")\n",
    "print(\"----\\t\\t--\\t\\t---\")\n",
    "for word, tf, idf in word_stats[:10]:\n",
    "    print(f\"{word:<12}\\t{tf:.2f}\\t\\t{idf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "beff3989-9636-4dcf-b1ab-f1e702da5b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Top Document:\n",
      " Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized. Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\n",
      "Established Large Language Models (LLM), what we call the foundation models, can learn in two ways:\n",
      " Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model's intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs. Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations?\n"
     ]
    }
   ],
   "source": [
    "tfidf_user_query = [user_query]\n",
    "new_tfidf_matrix = tfidf_vectorizer.transform(tfidf_user_query)\n",
    "tfidf_similarity_scores = cosine_similarity(new_tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "tfidf_top_doc_index = tfidf_similarity_scores.argmax()\n",
    "\n",
    "print(\"TF-IDF Top Document:\\n\", tfidf_documents[tfidf_top_doc_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4c6323ff-0143-41da-98a3-adb0130abb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc2vec_documents = [split.page_content for split in splits]\n",
    "doc2vec_tokenized_documents = [doc.lower().split() for doc in doc2vec_documents]\n",
    "\n",
    "doc2vec_tagged_documents = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(doc2vec_tokenized_documents)]\n",
    "doc2vec_model = Doc2Vec(doc2vec_tagged_documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "doc2vec_model.save(\"doc2vec_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a5a2c349-4aaf-4890-b36b-41b700507ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc2Vec Top Document:\n",
      " There are also generative models that generate images from text prompts, while others generate video from text prompts. There are other models that generate text descriptions from images. We will talk about these other types of models in Chapter 16, Going Beyond the LLM. But for most of the book, I felt it would keep things simple and let you focus on the core principles of RAG if we focus on the type of model that most RAG pipelines use, the LLM. But I did want to make sure it was clear, that while the book focuses primarily on LLMs, RAG can also be applied to other types of generative models, such as those for images and videos. Some popular examples of LLMs are the OpenAI ChatGPT models, the Meta LLaMA models, Google’s PaLM and Gemini models, and Anthropic’s Claude models. Foundation model\n",
      "A foundation model is the base model for most LLMs. In the case of ChatGPT, the foundation model is based on the GPT (Generative Pre-trained Transformer) architecture, and it was fine-tuned for Chat. The specific model used for ChatGPT is not publicly disclosed. The base GPT model cannot talk with you in chatbot-style like ChatGPT does. It had to get further trained to gain that skill.\n"
     ]
    }
   ],
   "source": [
    "loaded_doc2vec_model = Doc2Vec.load(\"doc2vec_model.bin\")\n",
    "\n",
    "doc2vec_document_vectors = [loaded_doc2vec_model.dv[str(i)] for i in range(len(doc2vec_documents))]\n",
    "doc2vec_user_query = [user_query]\n",
    "doc2vec_tokenized_user_query = [content.lower().split() for content in doc2vec_user_query]\n",
    "doc2vec_user_query_vector = loaded_doc2vec_model.infer_vector(doc2vec_tokenized_user_query[0])\n",
    "doc2vec_similarity_scores = cosine_similarity([doc2vec_user_query_vector], doc2vec_document_vectors)\n",
    "doc2vec_top_doc_index = doc2vec_similarity_scores.argmax()\n",
    "print(\"\\nDoc2Vec Top Document:\\n\", doc2vec_documents[doc2vec_top_doc_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8efc4030-3e3c-4006-b566-b4d1c4b18a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size of BERT (base-uncased) embeddings: 768\n",
      "\n",
      "BERT Top Document:\n",
      " Or if you are developing in a legal field, you may want it to sound more like a lawyer. Vector Store or Vector Database?\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "bert_documents = [split.page_content for split in splits]\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_vector_size = bert_model.config.hidden_size\n",
    "print(f\"Vector size of BERT (base-uncased) embeddings: {bert_vector_size}\\n\")\n",
    "\n",
    "bert_tokenized_documents = [bert_tokenizer(doc, return_tensors='pt', max_length=512, truncation=True) for doc in bert_documents]\n",
    "bert_document_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for doc in bert_tokenized_documents:\n",
    "        bert_outputs = bert_model(**doc)\n",
    "        bert_doc_embedding = bert_outputs.last_hidden_state[0, 0, :].numpy()\n",
    "        bert_document_embeddings.append(bert_doc_embedding)\n",
    "\n",
    "bert_user_query = [user_query]\n",
    "bert_tokenized_user_query = bert_tokenizer(bert_user_query[0], return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "bert_user_query_embedding = []\n",
    "with torch.no_grad():\n",
    "    bert_outputs = bert_model(**bert_tokenized_user_query)\n",
    "    bert_user_query_embedding = bert_outputs.last_hidden_state[0, 0, :].numpy()\n",
    "\n",
    "bert_similarity_scores = cosine_similarity([bert_user_query_embedding], bert_document_embeddings)\n",
    "\n",
    "bert_top_doc_index = bert_similarity_scores.argmax()\n",
    "\n",
    "print(\"BERT Top Document:\\n\", bert_documents[bert_top_doc_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1d13bc1d-b43d-4935-8391-4b7834589170",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_function)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1fa95831-fadf-4097-84ff-1ab85f504ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Document:\n",
      " Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized. Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\n",
      "Established Large Language Models (LLM), what we call the foundation models, can learn in two ways:\n",
      " Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model's intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs. Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations?\n"
     ]
    }
   ],
   "source": [
    "result = retriever.invoke(user_query)[0]\n",
    "\n",
    "print(\"\\nRetrieved Document:\\n\", result.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1db3a016-b04b-4ac5-86b1-9f039b4078bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"jclemens24/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba9c19d6-3964-4a45-adaf-21e72192d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Given the following question and retrieved context, determine if the context is relevant to the question.\n",
    "    Provide a score from 1 to 5, where 1 is not at all relevant and 5 is highly relevant.\n",
    "    Return ONLY the numeric score, without any additional text or explanation.\n",
    "\n",
    "    Question: {question}\n",
    "    Retrieved Context: {retrieved_context}\n",
    "\n",
    "    Relevance Score:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f5b534f9-cc8b-4ed6-b8a4-f7fd0a38f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(llm_output):\n",
    "    try:\n",
    "        score = float(llm_output.strip())\n",
    "        return score\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "def conditional_answer(x):\n",
    "    relevance_score = extract_score(x['relevance_score'])\n",
    "    if relevance_score < 4:\n",
    "        return \"I don't know\"\n",
    "    else:\n",
    "        return x['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5060a01-9b4a-4f8f-947e-27fe77c89493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First modify the format_docs to handle Document objects\n",
    "def format_docs(docs):\n",
    "    if hasattr(docs, 'page_content'):  # Single Document object\n",
    "        return docs.page_content\n",
    "    elif isinstance(docs, list):  # List of Document objects\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    elif isinstance(docs, dict):\n",
    "        return str(docs)\n",
    "    else:\n",
    "        return str(docs)\n",
    "\n",
    "# Modify the chain to properly handle Document objects\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough().assign(\n",
    "        context = lambda x: format_docs(x[\"context\"])\n",
    "    )\n",
    "    | RunnableParallel(\n",
    "        {\n",
    "            \"relevance_score\": (\n",
    "                RunnablePassthrough()\n",
    "                | (lambda x: relevance_prompt_template.format(\n",
    "                    question=str(x['question']),\n",
    "                    retrieved_context=x['context']  # Already formatted by previous step\n",
    "                  ))\n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            ),\n",
    "            \"answer\": (\n",
    "                RunnablePassthrough()\n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    | RunnablePassthrough().assign(final_answer = conditional_answer)\n",
    ")\n",
    "\n",
    "rag_chain_with_source = (\n",
    "    RunnableParallel({\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | RunnablePassthrough().assign(\n",
    "        answer = rag_chain_from_docs\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "85980ed4-a953-43f1-8de9-745bc9c9ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(retriever.invoke(user_query)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b72eb-34f1-4d75-991a-91d2ae59db08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
