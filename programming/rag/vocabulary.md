- LLM

Generating text using Generative AI tech

- Auto-encoding Model

- Auto-regressive Model

- Prompting

Sending a query to an LLM

- Prompt Design

Strategy implemented to design the prompt that is sent to LLM

- Prompt Engineering

Technical aspects related to the prompt used to improve the output from the LLM

- LangChain

Generative AI development framework for building RAG pipelines and LLMs within a pipeline. 

- LlamaIndex

Alternative to LangChain. Focus on search and retrieval tasks. Good option for advanced search or need to handle large datasets.

- Inference

The process of the LLM generating outputs or predictions based on given inputs using a pre-trained language model. The steps taken to provide an answer to a question is called inference.

- Context Window

The maximum number of tokens (words, sub-words or characters) that the model can process in a single pass. It determines the amount of text the model can see at once when generating responses.

- Sliding Window, Truncation

Dividing the input into smaller segments that fit withing the context window.

- Fine Tuning

Adjust the weights that define the model's intelligence based on new training data.

- Full Model Fine Tuning

Train a foundation model to gain new capabilities.

- Parameter Efficient Fine Tuning

A type of fine-tuning where parts of the parameters are fine tuned.

- Vector

A vector is a mathematical representation of data. In the context of natural language processing and LLMs they are called as embeddings.

Embeddings capture semantic information for tasks such as similarity search.


- Asymmetric Search


- Symmetric Search


- Semantic Search


- Overfitting


- Model Weight


- Cross-Encoder


- Bi-Encoder


- Chunk
