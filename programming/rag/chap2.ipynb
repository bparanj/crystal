{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f768e202-195c-410d-8755-a2442283d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USER_AGENT'] = 'RAGUuserAgent'\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a95b0e25-e0ad-41fe-bbdb-98b11bcb16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "str_output_parser = StrOutputParser()\n",
    "user_query = \"What are the advantages of using RAG?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7c24916-262b-4df8-a550-5157095566db",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=['https://kbourne.github.io/chapter1.html'],\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "623d4a16-c6b1-4f7f-9d4b-8562e71f352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(embedding_function)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90f0cad4-b3db-4208-aa53-70cf7fc4bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad367110-10ba-4c10-a076-0162c1ba9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50b850cc-6881-4acf-9ddb-3274db13c501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"jclemens24/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a159830-f645-4386-b9fc-9f216b271599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85aa3660-8c56-4c3e-9ab5-5e2602932018",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | str_output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7b62716-9680-495a-846b-a75e72c3fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}).assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aac905d6-b071-4776-a6be-debfdaafcc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'source': 'https://kbourne.github.io/chapter1.html'}, page_content='Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized. Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\\nEstablished Large Language Models (LLM), what we call the foundation models, can learn in two ways:\\n Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model\\'s intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs. Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations?'),\n",
       "  Document(metadata={'source': 'https://kbourne.github.io/chapter1.html'}, page_content='Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized. Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\\nEstablished Large Language Models (LLM), what we call the foundation models, can learn in two ways:\\n Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model\\'s intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs. Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations?'),\n",
       "  Document(metadata={'source': 'https://kbourne.github.io/chapter1.html'}, page_content='Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized. Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\\nEstablished Large Language Models (LLM), what we call the foundation models, can learn in two ways:\\n Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model\\'s intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs. Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations?'),\n",
       "  Document(metadata={'source': 'https://kbourne.github.io/chapter1.html'}, page_content='Maintaining this integration over time, especially as data sources evolve or expand, adds even more complexity and cost. Organizations need to invest in technical expertise and infrastructure to effectively leverage RAG capabilities while accounting for the rapid increase in complexities these systems bring with them. Potential for Information Overload: \\nIt is very possible for RAG-based systems to pull in too much information. It is just as important to implement mechanisms to address this issue as it is to handle times when not enough relevant information is found. Determining the relevance and importance of retrieved information to be included in the final output requires sophisticated filtering and ranking mechanisms. Without these, the quality of the generated content could be compromised by an excess of unnecessary or marginally relevant details. RAG Vocabulary#\\nNow is as good a time as any to review some vocabulary that should help you get familiar with the various concepts in RAG. This is not an exhaustive list, but understanding these core concepts should help you understand everything else we teach you about RAG in a more effective way:\\nLarge Language Model (LLM)\\nMost of this book will deal with LLMs. LLMs are generative AI technologies that focus on generating text.')],\n",
       " 'question': 'What are the advantages of using RAG?',\n",
       " 'answer': 'The advantages of using RAG (Retrieval-Augmented Generation) include:\\n\\n1. **Comprehensive Data Access**: RAG allows organizations to access and utilize all of their internal data effectively, which is particularly beneficial for larger companies that have vast amounts of data that are not readily accessible.\\n\\n2. **Enhanced Customer Insights**: By combining company data with knowledge of specific customer needs, RAG can provide deeper insights into customer interactions and preferences, leading to more personalized services.\\n\\n3. **Improved Decision-Making**: With the ability to pull in relevant data from various sources, RAG can support better decision-making by providing comprehensive information that was previously difficult to access.\\n\\n4. **Scalability**: RAG can help smaller companies leverage their internal data resources more effectively, enabling them to scale their operations and insights without needing extensive resources.\\n\\n5. **Potential for Innovation**: The integration of RAG with generative AI opens up new possibilities for innovation within organizations, allowing them to explore new ways of utilizing their data.\\n\\nOverall, RAG represents a significant advancement in how organizations can interact with and leverage their data for various applications.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = rag_chain_with_source.invoke(user_query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681dd097-53ab-4eb6-98aa-6a428f435f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
